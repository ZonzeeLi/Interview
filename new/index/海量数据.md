# 海量数据面试题

## 分而治之 

对于海量数据而言，由于无法一次性装进内存处理，导致我们不得不把海量的数据通过 hash 映射分割成相应的小块数据，然后再针对各个小块数据通过 hash_map 进行统计或其它操作。

那什么是 hash 映射呢？简单来说，就是为了便于计算机在有限的内存中处理 big 数据，我们通过一种映射散列的方式让数据均匀分布在对应的内存位置（如大数据通过取余的方式映射成小数存放在内存中，或大文件映射成多个小文件），而这个映射散列方式便是我们通常所说的 hash 函数，设计的好的 hash 函数能让数据均匀分布而减少冲突。

### 问题实例

#### 海量日志数据，提取出某日访问百度次数最多的那个IP

**分析：**百度作为国内第一大搜索引擎，每天访问它的 IP 数量巨大，如果想一次性把所有 IP 数据装进内存处理，则内存容量明显不够，故针对数据太大，内存受限的情况，可以把大文件转化成（取模映射）小文件，从而大而化小，逐个处理。换言之，先映射，而后统计，最后排序。

**解法：**具体分为以下3个步骤

1. 分而治之 / hash 映射

首先把这一天访问百度日志的所有 IP 提取出来，然后逐个写入到一个大文件中，接着采用映射的方法，比如 %1000，把整个大文件映射为 1000 个小文件。

2. hash_map 统计

当大文件转化成了小文件，那么我们便可以采用 hash_map(ip, value) 来分别对 1000 个小文件中的 IP 进行频率统计，再找出每个小文件中出现频率最大的 IP。

3. 堆 / 快速排序

统计出 1000 个频率最大的IP后，依据各自频率的大小进行排序（可采取堆排序），找出那个频率最大的 IP，即为所求。

注：Hash 取模是一种等价映射，不会存在同一个元素分散到不同小文件中去的情况，即这里采用的是 %1000 算法，那么同一个 IP 在 hash 后，只可能落在同一个文件中，不可能被分散的。

#### 寻找热门查询，300 万个查询字符串中统计最热门的 10 个查询

**原题：**搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个记录，请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

**分析：**这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。

根据分治思想，数据大则划为小的，例如一亿个 ip 求 Top 10，可先 %1000 将 ip 分到 1000 个小文件中去，并保证一种 ip 只出现在一个文件中，再对每个小文件中的 ip 进行 hash_map 统计并按数量排序，最后归并或者最小堆依次处理每个小文件的 top10 以得到最后的结果。

但对于本题，数据规模比较小，能一次性装入内存。因为根据题目描述，虽然有一千万个 Query，但是由于重复度比较高，故去除重复后，事实上只有 300 万的 Query，每个 Query 255Byte，因此我们可以考虑把他们都放进内存中去（300 万个字符串假设没有重复，都是最大长度，那么最多占用内存 3M * 1K / 4 = 0.75G。所以可以将所有字符串都存放在内存中进行处理）。

所以我们放弃分而治之 / hash 映射的步骤，直接上 hash_map 统计，然后排序。So，针对此类典型的 TOP K 问题，采取的对策往往是：hash_map + 堆。

**解法：**

1. hash_map 统计

先对这批海量数据预处理。具体方法是：维护一个 Key 为 Query 字串，Value 为该 Query 出现次数的 hash_map，即 hash_map(Query, Value)，每次读取一个 Query，如果该字串不在 Table 中，那么加入该字串，并将 Value 值设为 1；如果该字串在 Table 中，那么将该字串的计数加 1 即可。最终我们在 O(N) 的时间复杂度内用 hash_map 完成了统计；

2. 堆排序

借助堆这个数据结构，找出 Top K，时间复杂度为 N‘logK。即借助堆结构，我们可以在 log 量级的时间内查找和调整 / 移动。因此，维护一个 K（该题目中是 10）大小的小根堆，然后遍历 300 万的 Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O(n) + N' * O(logk），其中，N 为 1000 万，N’ 为 300 万。

当然，你也可以采用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小堆来对出现频率进行排序。

#### 有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词

由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略， 把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

**解法：**

1. 分而治之 / hash 映射

顺序读取文件，对于每个词 x，取 hash(x) % 5000，然后把该值存到 5000 个小文件（记为x0，x1，...，x4999）中。这样每个文件大概是 200k 左右。当然，如果其中有的小文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M。

2. hash_map 统计

对每个小文件，采用 trie 树 / hash_map 等统计每个文件中出现的词以及相应的频率。

3. 堆 / 归并排序

取出出现频率最大的 100 个词（可以用含 100 个结点的最小堆）后，再把 100 个词及相应的频率存入文件，这样又得到了 5000 个文件。最后就是把这 5000 个文件进行归并（类似于归并排序）的过程了。

#### 海量数据分布在 100 台电脑中，想个办法高效统计出这批数据的 TOP10

**解法一：**

如果同一个数据元素只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素：

1. 堆排序

在每台电脑上求出 TOP 10，可以采用包含 10 个元素的堆完成（TOP 10 小，用最大堆，TOP 10 大，用最小堆，比如求 TOP10 大，我们首先取前 10 个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整最小堆。最后堆中的元素就是 TOP 10 大）。

2. 组合归并

求出每台电脑上的 TOP 10 后，然后把这 100 台电脑上的 TOP 10 组合起来，共 1000 个数据，再利用上面类似的方法求出 TOP 10 就可以了。

**解法二：**

但如果同一个元素重复出现在不同的电脑中呢，比如拿两台机器求 Top 2 的情况来说，第一台的数据分布及各自出现频率为：a(50)，b(50)，c(49)，d(49) ，e(0)，f(0)，其中，括号里的数字代表某个数据出现的频率，如a (50) 表示 a 出现了 50 次。第二台的数据分布及各自出现频率为：a(0)，b(0)，c(49)，d(49)，e(50)，f(50)。

这个时候可以有两种办法：

- 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出 TOP 10，继而组合 100 台电脑上的 TOP 10，找出最终的 TOP 10。
- 或者，暴力求解。直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出 TOP 10。

#### 有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求你按照 query 的频度排序

**解法一：**

1. hash 映射

顺序读取 10 个文件，按照 hash(query)%10 的结果将 query 写入到另外 10 个文件（记为a0，a1，...，a9）中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。

2. hash_map 统计

找一台内存在 2G 左右的机器，依次对用 hash_map(query, query_count) 来统计每个 query 出现的次数。注：hash_map(query, query_count) 是用来统计每个 query 的出现次数，不是存储他们的值，出现一次，则 count+1。

3. 堆 / 快速 / 归并排序

利用快速 / 堆 / 归并排序按照出现次数进行排序，将排序好的 query 和对应的 query_count 输出到文件中，这样得到了 10 个排好序的文件。最后，对这10个文件进行归并排序（内排序与外排序相结合）。

**解法二：**

一般 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次性就可以加入到内存了。这样，我们就可以采用 trie 树 / hash_map 等直接来统计每个 query 出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

**解法三：**

与解法 1 类似，但在做完 hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如 MapReduce），最后再进行合并。

#### 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url？

**解法：**

可以估计每个文件的大小为 5G×64=320G，远远大于内存限制的 4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

1. 分而治之 / hash 映射

遍历文件 a，对每个 url 求取模 %1000，然后根据所取得的值将 url 分别存储到 1000 个小文件中。这样每个小文件的大约为 300M。遍历文件 b，采取和 a 相同的方式将 url 分别存储到1000小文件中。这样处理后，所有可能相同的 url 都在对应的小文件对中，不对应的小文件不可能有相同的 url。然后我们只要求出 1000 对小文件中相同的 url 即可。

2. hash_set 统计

求每对小文件中相同的 url 时，可以把其中一个小文件的 url 存储到 hash_set 中。然后遍历另一个小文件的每个 url，看其是否在刚才构建的 hash_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。

#### 100 万个数中找出最大的 100 个数

**解法一：**采用局部淘汰法。选取前 100 个元素，并排序，记为序列 L。然后一次扫描剩余的元素 x，与排好序的 100 个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把 x 利用插入排序的思想，插入到序列 L 中。依次循环，知道扫描了所有的元素。复杂度为 O(100万 * 100)。

**解法二：**采用快速排序的思想，每次分割之后只考虑比主元大的一部分，直到比主元大的一部分比 100 多的时候，采用传统排序算法排序，取前 100 个。复杂度为 O(100万*100)。

**解法三：**如果用一个含 100 个元素的最小堆完成。复杂度为 O(100万 * lg100)。

### 举一反三

#### 怎么在海量数据中找出重复次数最多的一个？

提示：先做 hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。

#### 上千万或上亿数据（有重复），统计其中出现次数最多的前 N 个数据。

提示：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用 hash_map / 搜索二叉树 / 红黑树等来进行统计次数。然后就是取出前 N 个出现次数最多的数据了，可以用堆完成。

#### 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析。

提示：这题是考虑时间效率。用 trie 树统计每个词出现的次数，时间复杂度是 O(n * le)（le表示单词的平均长度）。然后是找出出现最频繁的前10个词，可以用堆来实现，时间复杂度是 O(n * lg10)。所以总的时间复杂度，是 O(n * le) 与 O(n * lg10) 中较大的哪一个。

#### 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

提示：这题用 trie 树比较合适，hash_map 也行。当然，也可以先 hash 成小文件分开处理再综合。

#### 一个文本文件，找出前 10 个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。

提示：首先根据用 hash 并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件中 10 个最常出现的词。然后再进行归并处理，找出最终的 10 个最常出现的词。

## 外排序

### 问题实例

#### 给 10^7 个数据量的磁盘文件排序

输入：给定一个文件，里面最多含有 n 个不重复的正整数（也就是说可能含有少于 n 个不重复正整数），且其中每个数都小于等于 n，n = 10^7。 输出：得到按从小到大升序排列的包含所有输入的整数的列表。 条件：最多有大约 1MB 的内存空间可用，但磁盘空间足够。且要求运行时间在 5 分钟以下，10 秒为最佳结果。

**解法一：位图**

你可能会想到把磁盘文件进行归并排序，但题目要求你只有1MB的内存空间可用，所以，归并排序这个方法不行。熟悉位图的朋友可能会想到用位图来表示这个文件集合。例如正如编程珠玑一书上所述，用一个20位长的字符串来表示一个所有元素都小于20的简单的非负整数集合，边框用如下字符串来表示集合{1，2，3，5，8，13}：

```
0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0
```

上述集合中各数对应的位置则置1，没有对应的数的位置则置0。参考编程珠玑一书上的位图方案，针对我们的 10^7 个数据量的磁盘文件排序问题，我们可以这么考虑，由于每个 7 位十进制整数表示一个小于 1000 万的整数。我们可以使用一个具有 1000 万个位的字符串来表示这个文件，其中，当且仅当整数 i 在文件中存在时，第 i 位为 1。采取这个位图的方案是因为我们面对的这个问题的特殊性：

1. 输入数据限制在相对较小的范围内；
2. 数据没有重复；
3. 其中的每条记录都是单一的整数，没有任何其它与之关联的数据。

所以，此问题用位图的方案分为以下三步进行解决：

1. 将所有的位都置为0，从而将集合初始化为空；
2. 通过读入文件中的每个整数来建立集合，将每个对应的位都置为1；
3. 检验每一位，如果该位为1，就输出对应的整数。

第一次，只处理 1—4999999 之间的数据，这些数都是小于 5000000 的，对这些数进行位图排序，只需要约 5000000 / 8 = 625000Byte，也就是 0.625 M，排序后输出。

第二次，扫描输入文件时，只处理 4999999-10000000 的数据项，也只需要 0.625M（可以使用第一次处理申请的内存）。 因此，总共也只需要 0.625M，位图的方法有必要强调一下，就是位图的适用范围为针对不重复的数据进行排序，若数据有重复，位图方案就不适用了。

用此位图方法，严格说来还是不太行，空间消耗 10^7 / 8 还是大于 1M（1M=1024 * 1024空间，小于 10^7 / 8）。既然如果用位图方案的话，我们需要约 1.25MB（若每条记录是8位的正整数的话，则 10000000 / (102410248) ~= 1.2M）的空间，而现在只有 1MB 的可用存储空间，那么究竟该作何处理呢？

**解法二：多路归并**

分而治之，大而化小，也就是把整个大文件分为若干大小的几块，然后分别对每一块进行排序，最后完成整个过程的排序。k 趟算法可以在 kn 的时间开销内和 n / k 的空间开销内完成对最多 n 个小于 n 的无重复正整数的排序。

比如可分为 2 块（k=2，1趟反正占用的内存只有 1.25 / 2 M），1~4999999 和 5000000~9999999。

先遍历一趟，首先排序处理 1~4999999 之间的整数（用 5000000 / 8 = 625000个字的存储空间来排序 0~4999999 之间的整数），然后再第二趟，对 5000000~1000000 之间的整数进行排序处理。

## 多层划分

### 问题实例

#### 2.5 亿个整数中找出不重复的整数的个数，内存空间不足以容纳这 2.5 亿个整数

**分析：**有点像鸽巢原理，整数个数为 2^32，也就是说，我们可以将这 2^32 个数，划分为 2^8 个区域（比如用单个文件代表一个区域），然后将数据分离到不同的区域，然后不同的区域在利用 bitmap 就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。

#### 5 亿个 int 找它们的中位数

**分析：**首先我们将 int 划分为 2^16 个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。

实际上，如果不是 int 是 int64，我们可以经过 3 次这样的划分即可降低到可以接受的程度。即可以先将 int64 分成 2^24 个区域，然后确定区域的第几大数，在将该区域分成 2^20 个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有 2^20，就可以直接利用direct addr table进行统计了。

## BitMap

### 问题实例

#### 在 2.5 亿个整数中找出不重复的整数，内存不足以容纳这 2.5 亿个整数

**解法一：**采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 * 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5 亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。扫描完事后，查看 Bitmap，把对应位是 01 的整数输出即可。

**解法二：**也可进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

#### 给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那 40 亿个数当中？

**解法一：**可以用 Bitmap 的方法，申请 512M 的内存，一个 bit 位代表一个 unsigned int 值。读入 40 亿个数，设置相应的 bit 位，读入要查询的数，查看相应 bit 位是否为 1，为 1 表示存在，为 0 表示不存在。

## Trie 树

### 问题实例

#### 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，请给出思想，给出时间复杂度分析

提示：用 trie 树统计每个词出现的次数，时间复杂度是 O(n * le)（le表示单词的平均长度），然后是找出出现最频繁的前 10 个词。当然，也可以用堆来实现，时间复杂度是 O(n * lg10)。所以总的时间复杂度，是 O(n * le) 与 O(n * lg10) 中较大的哪一个。

#### 寻找热门查询

**原题：**搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为 1-255 字节。假设目前有一千万个记录，这些查询串的重复读比较高，虽然总数是 1 千万，但是如果去除重复和，不超过 3 百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的 10 个查询串，要求使用的内存不能超过 1G。

**提示：**利用 trie 树，关键字域存该查询串出现的次数，没有出现为 0。最后用 10 个元素的最小堆来对出现频率进行排序。