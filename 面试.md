

## 面试问题分类

### 操作系统



### 计算机网络

#### 基础内容

##### 1. TCP/IP 网络模型有几层？

- 应用层 : 为用户提供功能，比如HTTP、FTP、Telnet、DNS、SMTP等，当两个不同的设备需要通信，应用把数据传递给下一层。
- 传输层 : 为应用层提供网络支持，传输层的两种传输协议TCP和UDP。设备作为发送方，应用把数据给传输层再到下一层，作为接收方则反过来。
- 网络层 : 将数据从一个设备传输到另一个设备。最常使用的是IP协议。
- 网络接口层 : 为网络层提供「链路级别」传输的服务，负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。

##### 2. 键入网址到网页显示，期间发生了什么？

##### 3. Linux系统是如何收发网络包的？

###### 3.1 网络模型

OSI 七层模型 : 应用层、表示层、会话层、传输层、网络层、数据链路层、物理层
TCP/IP 四层模型 : 应用层、传输层、网络层、网络接口层

###### 3.2 Linux接收网络包的流程

当网卡接收到网络包后，通过DMA技术，将网络包放到Ring Buffer，当网络包到达，网卡发起硬件中断，执行中断处理函数，发出一个「软中断」来轮询处理数据，直到没有新数据恢复中断，一次中断处理多个网络包。

软中断处理网络包是从 Ring Buffer 中拷贝数据到内核 struct sk_buff 缓冲区中，从而可以作为一个网络包交给网络协议栈进行逐层处理。

首先，进入网络接口层，检查报文合法性，合法找到上层协议类型，比如是IPv4，或者是IPv6，去掉帧头和帧尾，到网络层。

取出IP包，判断网络包下一步是继续上层处理还是转发出去，如果网络包是发送给本机的，就会从IP头里查看再上一层使用的是TCP还是UDP，去掉IP头，到传输层。

取出TCP或UDP头，根据「源 IP、源端口、目的 IP、目的端口」找出对应的Socket，将数据拷贝到Socket的接收缓冲区。

最后，应用层调用Socket接口，从内核的Socket接收缓冲区读取新数据到应用层。

###### 3.3 Linux发送网络包的流程

和上述接收网络包的过程正好相反。

##### 4. TCP和UDP的区别？

#### HTTP

##### 1. HTTP 基本概念

###### 1.1 HTTP 是什么？

HTTP 是超文本传输协议。是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。

###### 1.2 HTTP 常见的状态码

- 200 OK : 常见的成功状态码，如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。
- 204 No Content : 常见的成功状态码，响应头没有 body 数据。
- 206 Partial Content : 是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。
- 301 Moved Permanently : 永久重定向。
- 302 Found : 临时重定向。说明请求的资源还在，但暂时需要用另一个 URL 来访问。301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。
- 304 Not Modified : 不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。
- 400 Bad Request : 客户端请求的报文有误。
- 403 Forbidden : 服务器禁止访问。
- 404 Not Found : 请求的资源在服务器上不存在或未找到，所以无法提供给客户端。
- 500 Internal Server Error : 服务器发生错误。
- 501 Not Implemented : 客户端请求的功能还不支持。
- 502 Bad Gateway : 通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。
- 503 Service Unavailable : 服务器当前很忙，暂时无法响应客户端。

###### 1.3 HTTP 常见的字段

- Host 字段 : 服务器的域名。
- Content-Length 字段 : 回应的数据长度。
- Connection 字段 : 最常用的是要求服务器使用 TCP 的持久连接，即 Keep-Alive 。HTTP/1.1 版本的默认连接都是持久连接。
- Content-Type 字段 : 用于服务器回应时，告诉客户端，本次数据是什么格式。
- Content-Encoding 字段 : 数据的压缩方法。表示服务器返回的数据使用了什么压缩格式。

##### 2. GET 与 POST

###### 2.1 GET 与 POST 的区别

GET 的语义是从服务器获取指定的资源，一般是写在 URL 中，浏览器会对 URL 的长度有限制。POST 的语义是根据请求负荷(报文 body )对指定的资源作出处理，不会对 body 大小做出限制。

###### 2.2 GET 和 POST 方法都是安全和幂等的吗？

安全 : 不会破坏服务器上的资源。
幂等 : 多次执行相同的操作，结果都是相同的。

- GET : 安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签。
- POST : 是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。所以，浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签。

不过实际过程中可能使用 GET 来实现新增或删除，POST 查询数据，这样的话就不一定了，而且虽然 POST 是用 body 传输数据，但是抓个包就可以看到数据，也不一定就比 GET 方法安全，所以要用 HTTPS 协议加密传输。

##### 3. HTTP 缓存技术

###### 3.1 强制缓存

强制缓存指的是只要浏览器判断缓存没有过期就直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。

强缓存使用的是两个 HTTP 响应头部字段实现，用来表示资源在客户端缓存的有效期。

- Cache-Control : 是一个相对时间。
- Expires : 是一个绝对时间。

如果头部同时存在这两个字段的话，Cache-Control 的优先级高于 Expires 。

Cache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：

- 当浏览器第一次请求访问服务器资源时，服务器返回资源的同时，响应头部会添加上 Cache-Control，这里设置了过期时间的大小。
- 当浏览器再次请求访问服务器资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，计算是否过期，如果没有，则使用缓存，否则重新请求。
- 服务器再次收到请求后，会更新响应头部的 Cache-Control。

###### 3.2 协商缓存

协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。

协商缓存基于两种头部来实现，第一种是请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段。

- 响应头部中的 Last-Modified : 表示这个响应资源的最后修改时间。
- 请求头部中的 If-Modified-Since : 当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上  Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行比较，如果最后修改时间较新，说明资源被修改过，返回最新资源；如果最后修改时间较早，说明无新修改，返回 304 走本地缓存。

第二种是请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段实现。

- 响应头部中的 Etag : 唯一标识响应资源。
- 请求头部中的 If-None-Match : 当本地缓存资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发送请求时，会将请求头中 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304 状态码，如果有变化返回 200。

第一种是基于时间实现的，第二种是基于唯一标识，相对来说后者更加准确的判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。

如果 HTTP 响应头部中同时存在 Etag 和 Last-Modified 字段的时候，Etag 的优先级更高，先判断 Etag 是否发生了变化，如果没有变化，则继续看 Last-Modified。

注意，协商缓存的字段需要配合强制缓存中 Cache-control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求。

使用 Etag 字段实现的协商缓存过程如下 : 

- 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在响应头部添加 Etag 唯一标识，这个唯一标识的值是根据当前请求的资源生成。
- 当浏览器再次请求访问服务器该资源时，首先检查强制缓存是否国企，如果没过期则直接使用本地缓存；如果过期，则在请求头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识。
- 服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较：
  - 如果值相等，则返回 304 Not Modified，不会返回资源；
  - 如果不相等，则返回 200 状态码和资源，并在响应头部加上新的 ETag 唯一标识；
- 如果浏览器收到的是 304 请求响应状态码，则会从本地缓存中加载资源，否则更新。

##### 4. HTTP 特性

###### 4.1 HTTP 1.1 的优点有哪些？

1. 简单

HTTP 的基本报文格式是 header + body ，头部信息也是 key-value 简单文本的形式，易于理解，降低了学习的门槛。

2. 灵活和易于扩展

HTTP 协议里的各类请求方法、URI / URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。

同时 HTTP 由于是工作在应用层( OSI 第七层)，则它下层可以随便变化。

HTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL / TLS 安全传输层，HTTP / 3把 TCP 层换成了基于 UDP 的 QUIC。

3. 应用广泛和跨平台

互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有跨平台的优越性。

###### 4.2 HTTP 1.1 的缺点有哪些？

1. 无状态双刃剑

无状态的好处，因为服务器不会记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。

无状态的坏处，既然服务器没有记忆能力，再完成有关联性的操作会很麻烦，比如一套购物流程，从登陆到下单到付款等，都需要验证用户身份，如果服务器不知道这些请求是关联的，每次都要验证一遍。比较简单的解决这一问题的方法就是 Cookie 技术。

Cookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。相当于，在客户端第一次请求后，服务器下发一个带有客户信息的字段，下次客户端请求服务器，带上该字段，服务器就能认得了。

2. 明文传输双刃剑

明文意味着传输中的信息是可方便阅读的，通过 F12 控制台或者抓包可以直接查看，为调试带来了极大的便利性。

不过这样也相当于信息裸奔，在传输的过程中很容易被窃取。

3. 不安全

HTTP 比较严重的缺点就是不安全 : 

- 通信使用明文，内容会被窃听。
- 不验证通信放的身份，会遭遇伪装。
- 无法验证报文的完整性，有可能被篡改。

###### 4.3 HTTP 1.1 的性能如何？

1. 长连接

早期 HTTP 1.0 性能上的一个很大问题，就是没发一次请求，都要新建一次 TCP 连接，而且是串行请求，做了无所谓的 TCP 连接和断开，增加了通信的开销。

HTTP 1.1 提出了长连接的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。

持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开。

2. 管道网络传输

在可在同一个 TCP 连接里面，客户端发起多个请求，只要第一个请求发出去了，不必等其回来，可以发送第二个请求出去，减少整体的响应时间。

但是服务器必须按照接收请求的顺序发送对这些管道化请求的响应。如果服务端在处理 A 请求耗时比较长，那么后续的请求处理都会被阻塞住，也就是队头阻塞。HTTP 1.1 解决了请求的队头阻塞，但是并没有解决响应的队头阻塞。

3. 队头阻塞

「请求 - 应答」的模式加剧了 HTTP 的性能问题。当顺序发送的请求序列，某一个请求因为某种原因被阻塞，后面的排队的请求就会一直被阻塞，这就是队头阻塞，类似于上班塞车。

##### 5. HTTP 与 HTTPS 

###### 5.1 HTTP 与 HTTPS 有哪些区别？

1. HTTP 是超文本传输协议，信息是明文传输，存在安全问题。HTTPS 则解决了 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL / TLS 安全协议，让报文加密传输。
2. HTTP 连接建立比较简单，TCP 三次握手之后即可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后要进行 SSL / TLS 握手过程，再进行加密的报文传输。
3. HTTP 的端口号是 80，HTTPS 的端口号是 443。
4. HTTPS 要向 CA（证书权威机构）申请数字证书，来保证服务器的身份可信。

###### 5.2 HTTPS 解决了 HTTP 的哪些问题？

HTTP 由于是明文传输，所以存在以下三个风险：

- 窃听风险，比如通信链路上可以获取通信内容。
- 篡改风险，比如强制植入垃圾广告。
- 冒充风险，比如冒充淘宝网站。

HTTPS 在 HTTP 和 TCP 之间加入了 SSL/TLS 协议，解决了以上问题。

- 信息加密 : 交互信息无法被窃取。
- 校验机制 : 无法篡改通信内容，篡改了就不能正常显示。
- 身份证书 : 证明网站时真正要浏览的网站。

###### 5.3 HTTPS 时如何解决以上的三个风险问题？

- 混合加密的方式实现了信息的机密性，解决了窃听的风险。
- 摘要算法的方式实现了完整性，为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。
- 将服务器公钥放入到数字证书中，解决了冒充的风险。

1. 混合加密

HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：

- 在通信建立之前使用非对称加密的方式交换「会话秘钥」，后续不再使用非对称加密。
- 在通信的过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。

采用「混合加密」的原因：

- 对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
- 非对称加密使用两个密钥，公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换的问题，但是速度会慢。

2. 摘要算法

摘要算法实现完整性，客户端发送明文前通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。

3. 数字证书

客户端向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用私钥解密。借助第三方权威机构 CA（数字证书认证机构），将服务器的公钥放在数字证书中，只要证书是可信的，公钥就是可信的。

###### 5.4 HTTPS 是如何建立连接的？其间交互了什么？

SSL / TLS 协议基本流程 :

- 客户端向服务器索要并验证服务器的公钥。
- 双方协商生产「会话秘钥」。
- 双方采用「会话秘钥」进行加密通信。

SSL / TLS 协议详细流程 :

1. ClientHello 

首先，客户端向服务器发起加密通信请求，即 ClientHello 请求。客户端主要发送以下信息给服务器：
（1）客户端支持的 SSL / TLS 协议版本。
（2）客户端生成的随机数 ( Client Random )，后面用户生成会话密钥条件之一。
（3）客户端支持的密码套件列表。

2. SeverHello

服务器收到客户端请求后，向客户端发出响应，即 SeverHello 。服务器回应的内容如下：
（1）确认 SSL / TLS 协议版本，如果浏览器不支持，则关闭加密通信。
（2）服务器生成的随机数 ( Server Random )，也是后面用户生产「会话秘钥」条件之一。
（3）确认的密码套件列表。
（4）服务器的数字证书。

3. 客户端回应

客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。

如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：
（1）一个随机数 ( pre-master key )。该随机数会被服务器公钥加密。
（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（3）客户端握手结束同值，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。

上面第一项的随机数是整个握手阶段的第三个随机数，会发送给服务端，所以这个随机数客户端和服务端都是一样的。

服务端和客户端有了这三个随机数 ( Client Random、Server Random、pre-master key )，接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。

4. 服务器的最后回应

服务器收到客户端的第三个随机数 ( pre-master key ) 之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后向客户端发送最后的信息如下：
（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内瓤的发生的数据做个摘要，用来供客户端校验。

至此，整个 SSL / TLS 的握手阶段全部结束。接下来客户端与服务器进入加密通信，使用的是普通的 HTTP 协议，只不过用「会话秘钥」加密内容。

![HTTPS流程图（图源小林coding）](https://github.com/ZonzeeLi/Interview/blob/master/picture/HTTPS%E6%B5%81%E7%A8%8B.png)

##### 6. HTTP /1.1、HTTP /2、HTTP /3 演变

###### 6.1 HTTP /1.1 相比 HTTP /1.0 提高了什么性能？

HTTP /1.1 相比 HTTP /1.0 性能上的改进：

- 使用 TCP 长连接的方式改善了 HTTP /1.0 短连接造成的性能开销。
- 支持管道( pipeline )网络传输，只要第一个请求发出去了，不必等其回来，就可以发送第二个请求，减少整体的响应时间。

HTTP /1.1 的性能瓶颈：

- 请求 / 响应头部 ( Header ) 未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 部分
- 发送冗长的首部。每次互相发送相同的首部造成过多浪费。
- 服务器是按请求的顺序响应的，如果服务器响应慢，则会招致客户端一直请求不到数据，也就是队头阻塞。
- 没有请求优先级控制。
- 请求只能从客户端开始，服务器只能被动响应。

###### 6.2 HTTP /2 做了什么优化？

HTTP /2 协议是基于 HTTPS 的，所以 HTTP /2 的安全性也是有保障的。

HTTP /2 相比 HTTP /1.1 性能上的改进：

1. 头部压缩

HTTP /2 会压缩头 ( Header )，如果同时发出多个请求，他们的头是一样的或者详细的，那么协议会消除重复的部分。

这就是 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高了速度。

2. 二进制格式

HTTP /2 不再像 HTTP /1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧 ( frame )：头信息帧 ( Headers Frame ) 和数据帧 ( Data Frame )。

这样虽然对人不友好，但是计算机只懂二进制，无需将明文的报文再转成二进制，而是直接解析，增加了数据传输的效率。

3. 数据流

HTTP /2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。

在 HTTP /2 中每个请求或响应的所有数据包，称为一个数据流 ( Stream )。每个数据流都标记着一个独一无二的编号 ( Stream ID )，不同的 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息。

客户端和服务器双方都可以建立 Stream，Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。

客户端还可以指定数据流的优先级，优先级高的请求，服务器就先响应该请求。

4. 多路复用

HTTP /2 是可以在一个连接中并发多个请求或响应，而不用按照顺序一一对应。

移除了 HTTP /1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，降低了延迟，大幅度提高了连接的利用率。

5. 服务器推送

HTTP /2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以主动向客户端发送消息。

HTTP /2 有什么缺陷：

HTTP /2 通过 Stream 的并发能力，解决了 HTTP /1 的队头阻塞的问题，看似很完美了，但是 HTTP /2 还是存在该问题，不过不是发生在 HTTP 这一层面，而是 TCP 这一层。

HTTP /2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP /2 队头阻塞问题。

所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。

###### 6.3 HTTP /3 做了哪些优化？

上述说明了 HTTP /1.1 和 HTTP /2 都有队头阻塞的问题：

- HTTP /1.1 中的管道 ( pipeline ) 虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按照顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应处理完这个请求后，才能处理下一个请求，属于 HTTP 层的队头阻塞。
- HTTP /2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。

HTTP /3 把 HTTP 下层的 TCP 协议改成了 UDP。

UDP 不管顺序，也不管丢包，不会出现 HTTP /2 队头阻塞的问题。基于 UDP 的 QUIC 协议可以实现类似 TCP 的可靠性传输。

QUIC 有以下3个特点：

1. 无队头阻塞

QUIC 协议也有类似 HTTP /2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为是一条 HTTP 请求。

QUIC 有自己的一套机制可以保证传输的可靠性。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。这与 HTTP /2 不同，HTTP /2 只要某个流中的数据包丢失了，其他流也会受到影响。

所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受到影响。

2. 更快的连接建立

对于 HTTP /1 和 HTTP /2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此他们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。

HTTP /3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。

但是 HTTP /3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS /1.3，因此仅需要 1 个 RTT 就可以「同时」完成建立连接与密钥协商。甚至在第二次连接的时候，数据包和 QUIC 握手信息 ( 连接信息 + TLS 信息 ) 一起发送，达到 0-RTT 的效果。

3. 连接迁移

基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接，那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。而建立连接的过程中包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络卡顿了一下，因此连接的迁移成本很高。

而 QUIC 协议没有四元组的方式来“绑定”连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此计是移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID，TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿，达到了连接迁移的效果。

所以 QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP /2 的多路复用协议。

##### 7. HTTP /1.1 如何优化？

可以从以下三种优化思路来优化 HTTP /1.1 协议：

- 尽量避免发送 HTTP 请求
- 在需要发送 HTTP 请求时，考虑如何减少请求次数
- 减少服务器的 HTTP 响应的数据大小

第一个思路是，通过缓存技术来避免发送 HTTP 请求。客户端收到第一个请求的响应后，可以将其缓存在本地磁盘，下次请求的时候，如果缓存没过期，就直接读取本地缓存的响应数据。如果缓存过期，客户端发送请求的时候带上响应数据的摘要，服务器比对后发现资源没有变化，就发出不带包体的 304 响应，告诉客户端缓存的响应仍然有效。

第二个思路是，减少 HTTP 请求的次数，有以下的方法：

1. 将原本由客户端处理的重定向请求，交给代理服务器处理，这样可以减少重定向请求的次数；
2. 将多个小资源合并成一个大资源再传输，能够减少 HTTP 请求次数以及 头部的重复传输，再来减少 TCP 连接数量，进而省去 TCP 握手和慢启动的网络消耗；
3. 按需访问资源，只访问当前用户看得到/用得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延迟请求，也就减少了同一时间的 HTTP 请求次数。

第三思路是，通过压缩响应资源，降低传输资源的大小，从而提高传输效率，所以应当选择更优秀的压缩算法。

###### 7.1 如何避免发送 HTTP 请求？

对于一些重复性的 HTTP 请求，比如每次请求得到的数据都一样的，可以把这些「请求-响应」的数据都缓存在本地，下次直接读取本地的数据，不必通过网络获取服务器的响应。所以避免 HTTP 请求的方法就是通过缓存技术。

客户端会把第一次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，而响应作为 value，两者形成映射关系。

服务器在发送 HTTP 响应时，会估算一个过期的时间，并且把这个信息放到响应头部，客户端在查看响应头部的信息时，一旦发送缓存的响应是过期的，则就会重新发送网络请求。而如果客户端第一次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，服务器上的资源没有变更，则服务器没有必要在响应中再带上这个资源。只需要客户端在重新发送请求时，在请求的 Etag 头部带上第一次请求响应的头部中的摘要，这个摘要时唯一标识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做比较。如果不同，说明客户端的缓存没有价值，重新带上资源，而如果相同，服务器返回不带有 body 的 304 Not Modified 响应，告诉客户端仍然有效，减少响应资源在网络中传输的延时。缓存真的是性能优化的一把万能钥匙，小到 CPU Cache、Page Cache、Redis Cache，大到 HTTP 协议的缓存。

###### 7.2 如何减少 HTTP 请求次数？

1. 减少重定向请求次数

服务器上的一个资源由于迁移维护等原因从一个地址迁至另一个地址，但是客户端不知道，继续请求第一个地址，这时服务器应当返回 302 响应码和 Location 头部，告诉客户端该资源迁至另一个地址了。

如果重定向请求越多，客户端发起的 HTTP 请求就越多。而且服务端一般不只有一台服务器，比如源服务器上一级是代理服务器，然后代理服务器与客户端进行通信，这时客户端重定向会导致客户端与代理服务器之间需要 2 次消息传递。如果重定向的工作交由代理服务器完成，就能减少 HTTP 请求次数了，而且当代理服务器知晓了重定向规则后，可以进一步减少消息传递次数。

2. 合并请求

如果把多个访问小文件的请求合并成一个大请求，虽然传输的总资源一样，但减少请求就减少了重复发送的 HTTP 头部。另外 HTTP /1.1 如果第一发送的请求阻塞，后续的请求就不会发送，为了防止单个请求阻塞，一般浏览器会同时发起 5-6 个请求，每一个都是不同的 TCP 连接，如果合并了请求，就会减少 TCP 连接的数量，省去了 TCP 握手和慢启动过程耗费的时间。

比如可以通过将多个小图片合并成一个大图片来减少 HTTP 请求的次数，以减少 HTTP 请求的次数，从而减少网络的开销。也有服务端使用 webpack 等打包工具将资源合并打包成大文件的方法。所以合并请求的方式就是合并资源，以一个大资源的请求替换多个小资源的请求。

但这样子的合并请求会带来新问题，当大资源中的某一个小资源发生改变后，客户端必须重新下载整个完整的大资源文件。

3. 延迟发送请求

一般 HTML 里会含有很多 HTTP 的 URL，当前不需要的资源，没必要请求，按需获取，减少第一时间的 HTTP 请求次数。请求网页时，没必要把全部资源获取到，只获取当前用户所看到的页面资源，当用户向下滑动页面的时候，再向服务器获取接下来的资源，达到延迟发送请求的效果。

###### 7.3 如何减少 HTTP 响应的数据大小？

1. 无损压缩

无损压缩是指资源经过压缩后，信息不被破坏，还能完全恢复到压缩前的原样，适合用在文本文件、程序可执行文件、程序源代码。

2. 有损压缩

有损压缩主要将次要的数据舍弃，牺牲一些质量来减少数据量、提高压缩比，这种方法经常用于压缩多媒体数据，比如音频、视频、图片。

##### 8. HTTPS RSA 握手解析

###### 8.1 RSA 握手过程

传统的 TLS 握手基本都是使用 RSA 算法来实现密钥交换的，在将 TLS 证书部署服务端时，证书文件中包含一对公私钥，其中公钥会在 TLS 握手阶段传递给客户端，私钥则一直留在服务端，一定要确保私钥不能被窃取。

在 RSA 密钥协商算法中，客户端会生成随机密钥，并使用服务端的公钥加密后再传给服务端。根据非对称加密算法，公钥加密的消息仅能通过私钥解密，这样服务端解密后，双方就得到了相同的密钥，再用它加密应用消息。

1. TLS 第一次握手

客户端首先会发一个「Client Hello」消息，有客户端使用的 TLS 版本号、支持的密码套件列表，以及生成的随机数 ( Client Random )，这个随机数会被服务端保留，是生成对称加密密钥的材料之一。

2. TLS 第二次握手

当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否支持，和从密码套件列表中选择一个密码套件，以及生成随机数 ( Server Random )。接着，返回「Server Hello」消息，消息里面有服务器确认的 TLS 版本号，也给出了随机数 ( Server Random )，然后从客户端的密码套件列表选择了一个合适的密码套件。客户端和服务端的两个随机数是后续作为生成「会话密钥」的条件，所谓的会话密钥就是数据传输时，所使用的对称加密密钥。

然后，服务端为了证明自己的身份，会发送「Server Certificate」给客户端，这个消息里含有数字证书。

随后，服务端发了「Server Hello Done」消息，目的是告诉客户端，我已经把该给你的东西都给你了，本次打招呼完毕。

3. TLS 第三次握手

客户端验证完证书后，可信则继续向下。接着，客户端会生成一个新的随机数 ( pre-master )，用服务器的 RSA 公钥加密该随机数，通过「Change Cipher Key Exchange」消息传给服务端。服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 ( pre-master )。

至此，客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master。根据这三个随机数生成会话密钥 ( Master Secret )，它是对称密钥，用于对后续的 HTTP 请求 / 响应的数据加解密。生成完会话密钥，然后客户端发一个「Change Cipher Spec」，告诉服务端开始使用加密方式发送消息。然后客户端再发一个「Encrypted Handshake Message ( Finishd ) 」消息，把之前所有发送的数据做个摘要，再用会话密钥 ( master secret ) 加密一下，让服务器做个验证，验证加密通信是否可用和之前握手信息是否有被中途篡改过。可以发现，「Change Cipher Spec」之前传输的 TLS 握手数据都是明文，之后都是对称密钥加密的密文。

4. TLS 第四次握手

服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双方都验证加密和解密没问题，那么握手正式完成。

最后，就用「会话密钥」加解密 HTTP 请求和响应了。

###### 8.2 RSA 算法的缺陷

使用 RSA 密钥协商算法的最大问题是不支持前向保密。

因为客户端传递随机数（用于生成对称加密密钥的条件之一）给服务端时使用的是公钥加密的，服务端收到到后，会用私钥解密得到随机数。所以一旦服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。为了解决这个问题，后面就出现了 ECDHE 密钥协商算法。

##### 9. HTTPS ECDHE 握手

###### 9.1 ECDHE 握手过程

1. TLS 第一次握手

客户端首先会发一个「Client Hello」消息，消息里面有客户端使用的 TLS 版本号、支持的密码套件列表，以及生成的随机数 ( Client Random )。

2. TLS 第二次握手

服务端收到客户端的「打招呼」，同样也要回礼，会返回「Server Hello」消息，消息面有服务器确认的 TLS 版本号，也给出了一个随机数 ( Server Random )，然后从客户端的密码套件列表选择了一个合适的密码套件。不过和 RSA 的密码套件所表示的内容不太一样。然后服务器为了证明自己的身份，发送「Certificate」消息，会把证书也发给客户端。这一步就和 RSA 握手过程有很大的区别，因为服务端选择了 ECDHE 密钥协商算法，所以会在发送完证书后，发送「Server Key Exchange」消息。随后，就是「Server Hello Done」消息。

至此，TLS 两次握手就已经完成了，目前客户端和服务端通过明文共享了这几个信息：Client Random、Server Random 、使用的椭圆曲线、椭圆曲线基点 G、服务端椭圆曲线的公钥，这几个信息很重要，是后续生成会话密钥的材料。

3. TLS 第三次握手

客户端收到了服务端的证书后，自然要校验证书是否合法，如果证书合法，那么服务端到身份就是没问题的。校验证书的过程会走证书链逐级验证，确认证书的真实性，再用证书的公钥验证签名，这样就能确认服务端的身份了，确认无误后，就可以继续往下走。

客户端会生成一个随机数作为客户端椭圆曲线的私钥，然后再根据服务端前面给的信息，生成客户端的椭圆曲线公钥，然后用「Client Key Exchange」消息发给服务端。最终的会话密钥，就是用「客户端随机数 + 服务端随机数 + x（ECDHE 算法算出的共享密钥） 」三个材料生成的。

算好会话密钥后，客户端会发一个「Change Cipher Spec」消息，告诉服务端后续改用对称算法加密通信。接着，客户端会发「Encrypted Handshake Message」消息，把之前发送的数据做一个摘要，再用对称密钥加密一下，让服务端做个验证，验证下本次生成的对称密钥是否可以正常使用。

4. TLS 第四次握手

最后，服务端也会有一个同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双方都验证加密和解密没问题，那么握手正式完成。于是，就可以正常收发加密的 HTTP 请求和响应了。

###### 9.2 RSA 和 ECDHE 握手过程的区别

- RSA 密钥协商算法「不支持」前向保密，ECDHE 密钥协商算法「支持」前向保密
- 使用 RSA 密钥协商算法，TLS 完成四次握手后，才能进行应用数据传输，而对于 ECDHE 算法，客户端可以不用等服务端的最后一次 TLS 握手，就可以提前发出加密的 HTTP 数据，节省了一个消息的往返时间
- 使用 ECDHE， 在 TLS 第 2 次握手中，会出现服务器端发出的「Server Key Exchange」消息，而 RSA 握手过程没有该消息

##### 10. HTTPS 如何优化？

###### 10.1 分析性能损耗

产生性能消耗的两个环节：

- 第一个环节，TLS 协议握手过程
- 第二个环节，握手后的对称加密报文传输

对于第二环节，现在主流的对称加密算法 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了硬件级别的优化，因此这个环节的性能消耗可以说非常地小。

对于第一个环节，TLS 协议握手过程不仅增加了网络延时 ( 最长可以花掉 2 RTT )，而且握手过程中的一些步骤也会产生性能损耗，比如：

- 对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥
- 客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销
- 双方计算 Pre-Master，也就是对称加密密钥

###### 10.2 硬件优化

HTTPS 协议是计算密集型，而不是 I/O 密集型，所以不能把钱花在网卡、硬盘等地方，应该花在 CPU 上。另外，如果可以，应该选择可以支持 AES-NI 特性的 CPU，因为这种款式的 CPU 能在指令级别优化了 AES 算法，这样便加速了数据的加解密传输过程。

###### 10.3 软件优化

软件优化的方向分为两种，一种是软件升级，一种是协议优化。软件升级就是把软件升级到最新的版本，而协议优化就是对「密钥交换过程」进行优化。

1. 密钥交换算法优化

RSA 密钥交换算法得 TLS 握手过程，不仅慢，而且安全性也不高。因此如果可以，尽量使用 ECDHE 密钥交换算法替换 RSA 算法，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，发送加密的应用数据，以此将 TLS 握手的消息往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性。

2. TLS 升级

如果可以，直接把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手。TLS1.3 对密码套件进行“减肥”了，对于密钥交换算法，废除了不支持前向安全性的 RSA 和 DH 算法，只支持 ECDHE 算法。

###### 10.4 证书优化

对证书的优化，可以有两个方向，一个是证书传输，一个是证书验证。

1. 证书传输优化

要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以，对于服务器的证书应该选择椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥长度比 RSA 短的多。

2. 证书验证优化

客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用 CA 公钥解密证书」以及「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性。

这个访问过程是 HTTP 访问，因此又会产生一系列网络通信的开销，如 DNS 查询、建立连接、收发数据等。

CRL 称为证书吊销列表 ( Certificate Revocation List )，这个列表由 CA 定期更新，列表内容是撤销信任的证书序号，如果服务器的证书在则会个列表中，说明证书失效，不在的话，认为证书有效。

- 第一个问题，由于 CRL 列表由 CA 维护，定期更新，如果一个证书被吊销后，客户端在更新 CRL 之前还是会信任这个证书，实时性较差
- 第二个问题，随着吊销证书的增多，列表会越来越大，下载速度会越来越慢，下载完客户端还要遍历列表，会导致校验证书这一环节的延时很大，拖慢了 HTTPS 连接。

现在基本使用 OCSP，即在线证书状态协议 ( Online Certificate Status Protocol )，来查询证书的有效性，它的工作方式是向 CA 发送查询请求，让 CA 返回证书的有效状态。

不必像 CRL 方式客户端需要下载大大的列表，还要从列表查询，同时因为可以实时查询每一张证书的有效性，解决了 CRL 的实时性问题。

OCSP 需要向 CA 查询，因此也是要发生网络请求，而且还得看 CA 服务器的“脸色”，如果网络状态不好，或者 CA 服务器繁忙，也会导致客户端在校验证书这一环节的延时变大。

为了解决这一网络开销，就出现了 OCSP Stapling，其原理是：服务器向 CA 周期性的查询证书状态，获得一个带有时间戳和签名的响应结果并缓存。当客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握手过程中发给客户端。由于有签名的存在，服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。

###### 10.5 会话复用

TLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那如果我们把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥，就可以减少 TLS 握手的性能损耗了。这种方式就是会话复用 ( TLS session resumption )，会话复用分为两种，一种是 Session ID，另一种是 Session Ticket。

1. Session ID

Session ID 的工作原理是，客户端和服务器首次 TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识，Session ID 和 会话密钥相当于 key-value 的关系。

当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后会在内存中找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信，为了安全性，内存中的会话密钥会定期失效。

但是有两个问题：

- 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，服务器的内存压力也会越大。
- 现在网站服务一般是由多台服务器通过负载均衡提供服务的，客户端再次连接不一定会命中上次访问过的服务器，于是还要走完整的 TLS 握手过程。

2. Session Ticket

为了解决 Session ID 的问题，就出现了 Session Ticket，服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端，类似于 HTTP 的 Cookie。

客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。

客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题就可以恢复会话，开始通信了。

对于集群的服务器，要确保每一台服务器加密「会话密钥」的密钥是一致的，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。

Session ID 和 Session Ticket 都不具备前向安全性，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。同时应对重放攻击也很困难。避免重放攻击的方式就是需要对会话密钥设定一个合理的过期时间。

3. Pre-shared Key

前面的 Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。对于重连 TLS 1.3 只需要 0 RTT，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 Pre-shared Key。同样的，Pre-shared Key 也有重放攻击的危险。

##### 11. HTTP /2 好在哪？

###### 11.1 兼容 HTTP /1.1

第一点，HTTP /2 没有在 URI 里引入新的协议名，仍然用 http:// 表示明文协议，用 https://  表示加密协议，于是只需要浏览器和服务器在背后自动升级协议，这样可以让用户意识不到协议的升级，很好的实现了协议的平滑升级。

第二点，只在应用层做了改变，还是基于 TCP 协议传输，应用层方面为了保持功能上的兼容，HTTP/2 把 HTTP 分解成了「语义」和「语法」两个部分，「语义」层不做改动，与 HTTP/1.1 完全一致，比如请求方法、状态码、头字段等规则保留不变。

但是，HTTP/2 在「语法」层面做了很多改造，基本改变了 HTTP 报文的传输格式。

###### 11.2 头部压缩

HTTP 协议的报文是由「Header + Body」构成的，对于 Body 部分，HTTP /1.1 协议可以使用头字段「Content-Encoding」指定 Body 的压缩方式，比如用 gzip 压缩，这样可以节约带宽，但报文中的另外一部分 Header 没有针对的优化方法。

HTTP /1.1 报文中 Header 部分存在的问题：

- 含很多固定的字段，比如Cookie、User Agent、Accept 等，这些字段加起来也高达几百字节甚至上千字节，所以有必要压缩。
- 大量的请求和响应的报文里有很多字段值是重复的，这样会使大量贷款被这些冗余的数据占用了，所以有必要避免重复性。
- 字段是 ASCII 编码的，虽然易于人类观察，但效率低，所以有必要改成二进制编码。

HTTP /2 没使用常见的 gzip 压缩方式来压缩头部，而是开发了 HPACK 算法，HPACK 算法主要包含以下组成：

- 静态字典
- 动态字典
- Huffman 编码 ( 压缩算法 )

客户端和服务器两端都会建立维护这个字典，用长度较小的索引号表示重复的字符串，再用 Huffman 编码压缩数据，可达到 50%~90% 的高压缩率。

###### 11.3 二进制帧

HTTP /2 厉害的地方在于将 HTTP /1 的文本格式改成二进制格式传输数据，极大提高了 HTTP 传输效率，而且二进制数据使用位运算能高效解析。

HTTP/2 把响应报文划分成了两个帧（Frame），HEADERS（首部）和 DATA（消息负载） 是帧的类型，也就是说一条 HTTP 响应，划分成了两个帧来传输，并且采用二进制来编码。

帧头（Frame Header）很小，只有 9 个字节，帧开头的前 3 个字节表示帧数据（Frame Playload）的长度。

帧长度后面的一个字节是表示帧的类型，HTTP/2 总共定义了 10 种类型的帧，一般分为数据帧和控制帧两类。

帧类型后面的一个字节是标志位，可以保存 8 个标志位，用于携带简单的控制信息，比如：

- END_HEADERS 表示头数据结束标志，相当于 HTTP/1 里头后的空行 ( "\r\n" )
- END_STREAM 表示单方向数据发送结束，后续不会再有数据帧
- PRIORITY 表示流的优先级

帧头的最后 4 哥字节是流标识符 ( Stream ID )，但最高位被保留不用，只有 31 位可以使用，因此流标识符的最大值是 2^31，大约是 21 亿，它的作用是用来标识该 Frame 属于哪个 Stream，接收方可以根据这个信息从乱序的帧里找到相同 Stream ID 的帧，从而有序组装信息。

最后面就是帧数据了，它存放的是通过 HPACK 算法压缩过的 HTTP 头部和 body。

###### 11.4 并发传输

HTTP /2 通过 Stream 的设计，多个 Stream 复用一条 TCP 连接，达到并发的效果，解决了 HTTP /1.1 队头阻塞的问题，提高了 HTTP 传输的吞吐量。

- 1 个 TCP 连接包含一个或者多个 Stream，Stream 是 HTTP/2 并发的关键技术
- Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成
- Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容 ( 头部和 body )

因此，HTTP 消息可以由多个 Frame 构成，以及 1 个 Frame 可以由多个 TCP 报文构成。

在 HTTP /2 连接上，不同 Stream 的帧是可以乱序发送的 ( 因此可以并发不同的 Stream )，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而同一 Stream 内部的帧必须是严格有序的。

客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。

同一个连接中 的 Stream ID 不能复用，只能顺序递增，所以当 Stream ID 耗尽时，需要发一个控制帧 GOAWAY，用来关闭 TCP 连接。

在 Nginx 中，可以通过 http2_max_concurrent_streams 配置来设置 Stream 的上限，默认是 128 个。

HTTP /2 通过 Stream 实现的并发，比 HTTP /1.1 通过 TCP 连接实现并发要牛逼的多，因为当 HTTP /2 实现 100 个并发 Stream 时，只需要建立一次 TCP 连接，而 HTTP /1.1 需要建立 100 个 TCP 连接，每个 TCP 连接都要经过 TCP 握手、慢启动以及 TLS 握手过程，这些都是很耗时的。

HTTP /2 还可以对每个 Stream 设置不同优先级，帧头中的「标志位」可以设置优先级，比如客户端访问 HTML / CSS 和图片资源时，希望服务器先传递 HTML / CSS，再传图片，那么就可以通过设置 Stream 的优先级来实现，以此提高用户体验。

###### 11.5 服务器主动推送资源

客户端发起的请求，必须使用的是奇数号 Stream，服务器主动的推送，使用的是偶数号 Stream。服务器在推送资源时，会通过 PUSH_PROMISE 帧传输 HTTP 头部，并通过帧中的 Promised Stream ID 字段告知客户端，接下来会在哪个偶数号 Stream 中发送 body。

###### 11.6 HTTP /2 总结

第一点，对于常见的 HTTP 头部通过静态表和 Huffman 编码的方式，将体积压缩了近一半，而且针对后续的请求头部，还可以建立动态表，将体积压缩近 90%，大大提高了编码效率，同时节约了带宽资源。

不过，动态表并非可以无限增大， 因为动态表是会占用内存的，动态表越大，内存也越大，容易影响服务器总体的并发能力，因此服务器需要限制 HTTP /2 连接时长或者请求次数。

第二点，HTTP /2 实现了 Stream 并发，多个 Stream 只需复用 1 个 TCP 连接，节约了 TCP 和 TLS 握手时间，以及减少了 TCP 慢启动阶段对流量的影响。不同的 Stream ID 才可以并发，即时乱序发送帧也没问题，但是同一个 Stream 里的帧必须严格有序。

另外，可以根据资源的渲染顺序来设置 Stream 的优先级，从而提高用户体验。

第三点，服务器支持主动推送资源，大大提升了消息的传输性能，服务器推送资源时，会先发送 PUSH_PROMISE 帧，告诉客户端接下来在哪个 Stream 发送资源，然后用偶数号 Stream 发送资源给客户端。

HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。

HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP /2 队头阻塞问题。

##### 12. HTTP /3 的优势

###### 12.1 HTTP /2 的问题

HTTP /2 的问题如下：

- 队头阻塞
- TCP 与 TLS 的握手时延
- 网络迁移需要重新连接

1. 队头阻塞

队头阻塞是HTTP /2 多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该 TCP 连接中的所有请求。

2. TCP 与 TLS 的握手时延

因为 TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使较高的 TCP 段已经被接受了，应用层也无法从内核中读取该部分数据，从 HTTP 的视角看，就是请求被阻塞了。

发送 HTTP 请求时，需要经过 TCP 三次握手和 TLS 四次握手 ( TLS 1.2 ) 的过程，因此共需要 3  个 RTT 的时延才能发出请求数据。而且 TCP 由于具有「拥塞控制」的特性，所以刚建立连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接产生"减速"效果。

3. 网络迁移需要重新连接

一个 TCP 连接是由四元组 ( 源 IP 地址、源端口、目标 IP 地址、目标端口 ) 确定的，这意味着如果 IP 地址或者端口变动了，会导致 TCP 和 TLS 重新握手，这不利于移动设备切换网络的场景，比如 4G 切换成 WIFI。

###### 12.2 QUIC 协议的特点

UDP 是一个简单、不可靠的传输协议，而且是 UDP 包之间是无序的，也没有依赖关系。而且，UDP 是不需要连接的，也就不需要握手和挥手的过程，所以天然的就比 TCP 快。HTTP /3 不仅仅是简单将传输协议换成了 UDP，还基于 UDP 协议在「应用层」实现了 QUIC 协议，它具有类似 TCP 的连接管理、拥塞窗口、流量控制的网络特性，相当于将不可靠传输的 UDP协议变成可靠的，所以不用担心数据包丢失的问题。

1. 无队头阻塞

QUIC 协议也有类似 HTTP /2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为是一条 HTTP 请求。

由于 QUIC 使用的传输协议是 UDP，UDP 不关心数据包的顺序，如果数据包丢失，UDP 也不关心。不过 QUIC 协议会保证数据包的可靠性，每个数据包都有一个序号唯一标识，当某个流中的一个数据包丢失了，即使该流的其他数据包到达了，数据也无法被 HTTP /3 读取，直到 QUIC 重传丢失的报文，数据才会交给 HTTP /3。而其他流的数据报文只要被完整接收，HTTP/3 就可以读取到数据。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。

所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。

2. 更快的连接建立

对于 HTTP /1 和 HTTP /2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。

HTTP /3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。但是 HTTP  /3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS 1.3，因此仅需要 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息 ( 连接信息 + TLS 信息 ) 一起发送，可以做到 0 RTT 的效果。

3. 连接迁移

QUIC 协议没有用四元组的方式来“绑定”连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标识自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息 ( 比如连接 ID，TLS 密钥等 )，就可以无缝地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。

###### 12.3 HTTP /3 协议

HTTP /3 同 HTTP /2 一样采用二进制帧的结构，不同的地方在于 HTTP /2 的二进制帧里需要定义 Stream，而 HTTP /3 自身不需要再定义 Stream，直接使用 QUIC 里的 Stream，于是 HTTP /3 的帧的结构也变简单了。

HTTP /3 的帧头只有两个字段：类型和长度。根据帧类型的不同，大体上分为数据帧和控制帧两大类，HEADERS 帧 ( HTTP 头部 ) 和 DATA 帧 ( HTTP body ) 属于数据帧。HTTP /3 在头部压缩算法这一方面也做了升级，升级成了 QPACK。与 HTTP /2 中的 HPACK 编码方式相似，HTTP /3 中的 QPACK 也采用了静态表、动态表及 Huffman 编码。

#### TCP 

##### 1. TCP 三次握手与四次挥手

###### 1.1 TCP 基本认识

首先看一下 TCP 的头部格式，

![TCP 头部格式。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/TCP%E5%A4%B4%E9%83%A8%E6%A0%BC%E5%BC%8F.png)

序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。

确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。

控制位：

- ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。
- RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。
- SYN：该位为 1 时，标识希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。
- FIN：该位为 1 时，标识今后不再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。

IP 层是不可靠的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。如果需要保障网络数据包的可靠性，那么就需要由上层 ( 传输层 ) 的 TCP 协议来负责。因为 TCP 是一个工作在传输层的可靠数据传输的服务，它能确保接收端接受的网络包是无损坏、无间隔、非冗余和按序的。

TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。

- 面向连接：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的；
- 可靠的：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端；
- 字节流：用户消息通过 TCP 协议传输时，消息可能会被操作系统「分组」成多个的 TCP 报文，如果接收方的程序如果不知道「消息的边界」，是无法读出一个有效的用户消息的。并且 TCP 报文是「有序的」，当「前一个」TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对「重复」的 TCP 报文会自动丢弃。

而连接的定义为，用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接。

所以建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识。

- Socket：由 IP 地址和端口号组成
- 序列号：用来解决乱序问题等
- 窗口大小：用来做流量控制

TCP 四元组可以唯一的确定一个连接，四元组包括源地址、源端口、目的地址、目的端口。

源地址和目的地址的字段 ( 32 位 ) 是在 IP 头部中，作用是通过 IP 协议 发送报文给对方主机。

源端口和目的端口的字段 ( 16 位 ) 是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。

有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？

服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，客户端 IP 和 端口是可变的，其理论值计算位，最大 TCP 连接数 = 客户端的 IP 数 × 客户端的端口数。

对 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是服务端单机最大 TCP 连接数，约为 2 的 48 次方。但是服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：

- 文件描述符限制。每个 TCP 连接都是一个文件，如果文件描述符被沾满了，会发生 too many open files。Linux 对可打开的文件描述符数量分别作了三个方面的限制：
	- 系统级：当前系统可打开的最大数量，通过 cat /proc/sys/fs/file-max 查看；
	- 用户级：指定用户可打开的最大数量，通过 cat /etc/security/limits.conf 查看；
	- 进程级：单个进程可打开的最大数量，通过 cat /proc/sys/fs/nr_open 查看；
- 内存限制。每个 TCP 连接都要占用一定的内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。

UDP 和 TCP 有什么区别呢？分别的应用场景是？

UDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。而且 UDP 的协议非常简介，头部只有 8 个字节 ( 64 位 )：

- 目标和源端口：主要是告诉 UDP 协议应该把报文发送给哪个进程。
- 包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。
- 校验和：校验和是为了提供可靠的 UDP 首部和数据而设计，防止收到在网络传输中受损的 UDP 包。

TCP 和 UDP 区别：

1. 连接

- TCP 是面向连接的传输层协议，传输数据前先要建立连接。
- UDP 是不需要连接，即刻传输数据。

2. 服务对象

- TCP 是一对一的两点服务，即一条连接只有两个端点。
- UDP 支持一对一、一对多、多对多的交互通信。

3. 可靠性

- TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。
- UDP 是尽最大努力交付，不保证可靠交付数据。

4. 拥塞控制、流量控制

- TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
- UDP 则没有，即使网络非常拥堵，也不会影响 UDP 的发送速率。

5. 首部开销

- TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。
- UDP 首部只有 8 个字节，并且是固定不变的，开销很小。

6. 传输方式

- TCP 是流式传输，没有边界，但保证顺序和可靠。
- UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。

7. 分片不同

- TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。
- UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。

TCP 和 UDP 应用场景：

由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：

- FTP 文件传输
- HTTP / HTTPS

由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于：

- 包总量较少的通信，如 DNS、SNMP 等
- 视频、音频等多媒体通信
- 广播通信

为什么 UDP 头部没有「首部长度」字段，而 TCP 头部有「首部长度」字段呢？

原因是 TCP 有可变长的「选项」字段，而 UDP 头部长度则是不会变化的，无需多一个字段去记录 UDP 的首部长度。

为什么 UDP 头部有「包长度」字段，而 TCP 头部则没有「包长度」字段呢？

TCP 的负载数据长度计算为：TCP 的数据长度 = IP 总长度 - IP 首部长度 - TCP 首部长度

其中 IP 总长度和 IP 首部长度，在 IP 首部格式是已知的。TCP 首部长度，则是在 TCP 首部格式已知的，所以可以计算出 TCP 数据的长度。但其实 UDP 也同样可以计算，因为为了网络设备硬件设计和处理方便，首部长度需要是 4 字节的整数倍。如果去掉该字段的话，那首部长度就不是 4 字节的整数倍了。

###### 1.2 TCP 连接建立

TCP 三次握手的过程如下图：

![TCP 三次握手。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png)

- 一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。
- 三次握手的第一个报文 SYN 报文：客户端会随机初始化序号 ( client_isn )，将此序号置于 TCP 首部的「序列号」字段中，同时把 SYN 标志位置为 1，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。
- 三次握手的第二个报文 SYN + ACK 报文：服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号 ( server_isn )，将此序号填入 TCP 的首部的「序列号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1，接着把 SYN 和 ACK 标志位置为 1，最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。
- 三次握手的第三个报文 ACK 报文：客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1，其次「确认应答号」字段填入 server_isn + 1，最后把报文发送给服务端，这一次报文可以携带客户端到服务器的数据，之后客户端处于 ESTABLISHED 状态。
- 服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。

比较常问的则是前两次握手是不可以携带数据的，第三次握手是可以携带数据的。一旦完成握手，双方都处于 ESTABLISHED 状态，此时连接就建立完成，可以相互发送数据了。

在 Linux 中查看 TCP 的状态通过 netstat -napt 命令查看。

为什么是三次握手？

因为三次握手才能保证双方具有接收和发送的能力。连接是用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。所以三次握手才可以初始化 Socket、序列号和窗口大小并建立 TCP 连接。以三个方面分析三次握手的原因：

- 三次握手才可以阻止重复历史连接的初始化 ( 主要原因 )
- 三次握手才可以同步双方的初始序列号
- 三次握手才可以避免资源浪费

1. 避免历史连接

为了防止旧的重复连接初始化造成混乱。客户端连续发送多次 SYN 建立连接的报文，在网络拥堵的情况下：

- 一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端
- 那么此时服务端会回一个 SYN + ACK 报文给客户端
- 客户端收到后可以根据自身的上下文，判断这是一个历史连接 ( 序列号过期或超时 )，那么客户端就会发送 RST 报文给服务端，表示中止这一次连接。

如果是两次握手连接，就无法组织历史连接，因为在两次握手的情况下，「被动发起方」没有中间状态给「主动发起方」来阻止历史连接，导致「被动发起方」可能建立一个历史连接，造成资源浪费。要解决这种现象，最好就是在「被动发起方」发送数据前，也就是建立连接之前，要阻止掉历史连接，这样就不会造成资源浪费，而要实现这个功能，就需要三次握手。

2. 同步双方初始序列号

TCP 协议的通信双方，都必须维护一个「序列号」，序列号是可靠传输的一个关键因素，它的作用是：

- 接收方可以去除重复的数据
- 接收方可以根据数据包的序列号按序接收
- 可以标识发送出去的数据包中，哪些是已经被对方收到的 ( 通过 ACK 报文中的序列号知道 )

当客户端发送携带「初始序列号」的 SYN 报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样一来一回，才能确保双方的初始序列号能被可靠的同步。两次握手只保证了一方的初始序列号被对方接收，没办法保证双方的都被确认。

3. 避免资源浪费

如果客户端的 SYN 阻塞了，重复发送多次 SYN 报文，那么服务器在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。

为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？

- 为了防止历史报文被下一个相同四元组的连接接收 ( 主要方面 )
- 为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收

假设每次建立连接，客户端和服务端的初始化序列号都是从 0 开始，客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，而此时服务端的进程重启了，于是就会发送 RST 报文来断开连接。紧接着，客户端又与服务端建立了与上一个连接相同四元组的连接，在新连接建立完成，上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口，所以该数据包会被服务端正常接收，造成混乱。所以如果每次建立连接，客户端和服务端的初始化序列号都是一样的话，很容易出现历史报文被下一个相同四元组的连接接收的问题。

为什么 TCP 层还需要 MSS 呢？

- MTU：一个网络包的最大长度，以太网中一般为 1500 字节。
- MSS：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度。

当 IP 层有一个超过 MTU 大小的数据 ( TCP 头部 + TCP 数据 ) 要发送，那么 IP 层就要进行分片，把数据分片成若干份，由目标主机的 IP 层来进行重新组装，再交给上一层 TCP 传输层。但如果有一个 IP 分片丢失，整个 IP 报文的所有分片都要重传，因为 IP 没有超时重传机制，它由传输层的 TCP 来负责超时和重传。所以如果整个 TCP 报文也要交给 IP 层进行超时全部重传是非常没有效率的。

所以，为了达到最佳的传输效能，TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，就会先进行分片，由它形成的 IP 包长度就不会大于 MTU，自然不用 IP 分片了。经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。

三次握手丢失，会发生什么？

1. 第一次握手丢失

当客户端想和服务端建立 TCP 连接的时候，首先第一个发的是 SYN 报文，然后进入到 SYN_SENT 状态，如果客户端迟迟收不到服务端的 SYN-ACK ( 第二次握手 )，就会触发超时重传机制，重传 SYN 报文。不同版本的操作系统可能超时时间不同，这个是写死在内核里的，而且对于重发次数来讲，在 Linux 里，客户端的 SYN 报文最大重传次数由 tcp_syn_retries 内核参数控制，这个参数是可以自定的，一般为 5，通常每次超时时间是上一次的 2 倍，当第 5 次超时重传，如果服务端仍然没有回应 ACK，客户端就不再发送 SYN 包，然后断开 TCP 连接。

2. 第二次握手丢失

当服务端收到的第一次握手后，回 SYN-ACK 给客户端，此时服务端进入 SYN_RCVD 状态。如果第二次握手客户端迟迟没有收到，那么客户端会认为自己的 SYN 报文丢失了，这样客户端会触发超时重传机制，重传 SYN 报文。而且因为客户端如果收到第二次握手的报文会发送 ACK 给服务端，但如果第二次握手报文丢失，服务端收不到第三次握手，会触发超时重传机制，重传 SYN-ACK 报文。在 Linux 下，SYN-ACK 报文的最大重传次数由 tcp_synack_retries内核参数决定，默认值是 5。

3. 第三次握手丢失

客户端收到服务端的 SYN-ACK 后，会回一个 ACK 报文，此时客户端进入 ESTABLISH 状态。如果第三次握手丢失了，服务端收不到确认报文，就会触发超时重传机制。

注意 ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。

SYN 攻击

TCP 连接需要三次握手，假设攻击者短时间内伪造不同的 IP 地址的 SYN 报文，服务端没收到一个 SYN 报文，就进入 SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之占满服务端的半连接队列，使得服务器不能为正常用户服务。

避免 SYN 攻击方式一：

通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包，控制这些队列的最大值为 net.core.netdev_max_backlog，SYN_RCVD 状态连接的最大个数为 net.ipv4.tcp_max_syn_backlog，当队列满，对新的 SYN 直接回 RST，丢弃连接，net.ipv4.tcp_abort_on_overflow。

避免 SYN 攻击方式二：

Linux 内核的 SYN 队列 ( 半连接队列 ) 和 Accpet 队列 ( 全连接队列 ) 的工作流程如下：

- 当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「 SYN 队列」
- 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文
- 服务端接收到 ACK 报文后，从「 SYN 队列」移除放入到「 Accept 队列」
- 应用通过调用 accpet() socket 接口，从「 Accept 队列」取出连接

如果应用程序过慢。就会导致「 Accept 队列」被占满。如果受到了 SYN 攻击：

- 如果不断受到 SYN 攻击，就会导致 SYN 队列 ( 半连接队列 ) 被占满，从而导致无法再建立新的连接。tcp_syncookies 的方式可以应对 SYN 攻击的方法：net.ipv4.tcp_syncookies = 1
- 当「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」
- 计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」返回客户端
- 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」
- 最后应用通过调用 accpet () socket 接口，从「 Accept 队列」取出的连接

###### 1.3 TCP 连接断开

TCP 断开连接是通过四次挥手的方式，双方都可以主动断开连接，断开连接后主机中的「资源」将被释放。

![TCP 四次挥手。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B.png)

- 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位置为 1 的报文，即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态
- 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状态
- 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态
- 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态
- 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态
- 服务器收到了 ACK 应答报文后，就进入了 CLOSED 状态，至此服务端已经完成连接的关闭
- 客户端在经过了 2MSL 一段时间后，自动进入 CLOSED 状态，至此客户端也完成连接的关闭。

每个方向都需要一个 FIN 和一个 ACK，需要注意只有主动关闭来连接的才有 TIME_WAIT 状态。

为什么挥手需要四次？

- 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接受数据。
- 服务器收到客户端得 FIN 报文时，先回一个 ACK 应答报文，但是服务器可能仍然还有数据处理和发送，等服务端不再发数据，才发送 FIN 报文给客户端来表示 同意关闭现在的连接。

第一次挥手丢失了，会发生什么？

当客户端调用 close 函数后，就会向服务端发送 FIN 报文，试图与服务端断开连接，此时客户端进入到 FIN_WAIT_1 状态。如果正常搜狐到服务端的 ACK ，就会变成 FIN_WAIT_2 状态，如果丢失了，就会收不到 ACK，触发超时重传机制，重传 FIN 报文，重发次数由 tcp_orphan_retries 参数控制，当重传次数超过该值后，就不再发送 FIN 报文，直接进入到 close 状态。

第二次挥手丢失了，会发生什么？

当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 CLOSE_WAIT 状态。但 ACK 报文是不会重传的，所以客户端会触发超时重传机制，重传 FIN 报文。

当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 FIN_WAIT_2 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。FIN_WAIT_2 状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长，默认值是 60 秒。如果在 60 秒后还没有收到 FIN 报文，客户端的连接就会直接关闭。

但是注意，如果主动关闭方使用 shutdown 函数关闭连接且指定只关闭发送方向，而接收方向并没有关闭，那么意味着主动关闭方还是可以接收数据的。如果主动关闭方一直没收到第三次挥手，那么主动关闭方的连接将会一直处于 FIN_WAIT_2 状态 ( tcp_fin_timeout 无法控制 shutdown 关闭的连接 )。

第三次挥手丢失了，会发生什么？

当服务端收到客户端的 FIN 报文后，内核会自动回复 ACK，同时连接处于 CLOSE_WAIT 状态，它表示等待应用进程调用 close 函数关闭连接。此时，内核是没有权利替代进程关闭连接，必须由进程主动调用 close 函数来触发服务端发送 FIN 报文。服务端处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文，同时连接进入 LAST_ACK 状态，等待客户端返回 ACK 来确认连接关闭。如果收不到这个 ACK，服务端就会重发 FIN 报文，重发次数由 tcp_orphan_retries 参数控制。

第四次挥手丢失了，会发生什么？

当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 TIME_WAIT 状态。在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭状态。然后，服务端因为没有接收到 ACK 报文，会处于 LAST_ACK 状态，服务端会重发 FIN 报文，重发次数由 tcp_orphan_retries 控制。

为什么 TIME_WAIT 等待的时间是 2MSL？

MSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。

MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL 消耗为 0 的时间，以确保报文已被自然消亡。

TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了。

TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以一来一回需要等待 2 倍的时间。

比如，如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 FIN 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。

可以看到 2MSL时长 这其实是相当于至少允许报文丢失一次。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。

2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 2MSL 时间将重新计时。

在 Linux 系统里 2MSL 默认是 60 秒，那么一个 MSL 也就是 30 秒。Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。

为什么需要 TIME_WAIT 状态？

主动发起关闭连接的一方，才会有 TIME-WAIT 状态。需要 TIME-WAIT 主要是两个原因：

1. 防止历史连接中的数据，被后面相同四元组的连接错误的接收

首先来了解序列号 ( SEQ ) 和初始序列号 ( ISN )，

- 序列号，是 TCP 一个头部字段，标识了 TCP 发送端到 TCP 接收端的数据流的一个字节，因为 TCP 是面向字节流的可靠协议，为了保证消息的顺序性和可靠性，TCP 为每个传输方向上的每个字节都赋予了一个编号，以便于传输成功后确认、丢失后重传以及在接收端保证不会乱序。序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0。
- 初始序列号，在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时。

序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据。

假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？

服务端在关闭连接之前发送的 SEQ = x 报文，被网络延迟了，接着服务端以相同的四元组重新打开了新连接，即重传，前面被延迟的 SEQ = x 这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 2MSL 时长，这个时间足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。

2. 保证「被动关闭连接」的一方，能被正确的关闭

TIME-WAIT 为了等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。如果客户端最后一次 ACK 报文在网络中丢失了，服务端会重发 FIN 报文，假设没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSED 状态，如果该 ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。服务端收到这个 RST 并将其解释为一个错误 ( Connection reset by peer )，这对于一个可靠的协议来说不是一个优雅的终止方式。为了防止这种情况出现，客户端必须等待足够长的时间确保服务端收到 ACK，如果服务端没有收到 ACK，那么就会触发 TCP 重传机制，服务端会重新发送一个 FIN，这样一去一来刚好两个 MSL 的时间。

TIME_WAIT 过多有什么危害？

- 内存资源占用过多
- 对端口资源的占用过多，一个 TCP 连接至少消耗「发起连接方」的一个本地端口。如果「发起连接方」的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。

客户端受端口资源限制：客户端 TIME_WAIT 过多，就会导致端口资源被占用，因为端口就 65536 个，被占满就会导致无法创建新的连接。

服务端受端口资源限制：由于一个四元组表示 TCP 连接，理论上服务端可以建立很多连接，因为服务端只监听一个端口，不会因为 TCP 连接过多而导致端口资源受限。但是 TCP 连接过多，会占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等。

如何优化 TIME_WAIT？

下面的几个优化 TIME_WAIT 的几个方式，都是有利有弊：

- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项
- net.ipv4.tcp_max_tw_buckets
- 程序中使用 SO_LINGER，应用强制使用 RST 关闭。

1. net.ipv4.tcp_tw_reuse 和 tcp_timestamps

net.ipv4.tcp_tw_reuse  Linux 内核参数开启后，则可以复用处于 TIME_WAIT 的 socket 为新的连接所用。tcp_tw_reuse 功能只能用客户端 ( 连接发起方 )，因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。使用这个选项，还有一个前提，打开对 TCP 时间戳的支持，即 net.ipv4.tcp_timestamps = 1，这个时间戳的字段是在 TCP 头部的「选项」里，它由一共 8 个字节表示时间戳，其中第一个 4 字节字段用来保存发送该数据包的时间，第二个 4 字节字段用来保存最近一次接收对方发送到达数据的时间。由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。

2. net.ipv4.tcp_max_tw_buckets

这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置，这个方法比较暴力。

3. 程序中使用 SO_LINGER

设置 socket 选项，来设置调用 close 关闭连接行为。如果 l_onoff 为非 0， 且 l_linger 值为 0，那么调用 close 后，会立该发送一个 RST 标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了 TIME_WAIT 状态，直接关闭。

如果服务端要避免过多的 TIME_WAIT 状态的连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT。

如果已经建立了连接，但是客户端突然出现故障了怎么办？

TCP 有一个机制是保活机制，这个机制的原理如下：

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔。如果开启了 TCP 保活，需要考虑以下几种情况：

- 第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。
- 第二种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，会产生一个 RST 报文，这样很快就会发现 TCP 连接已经被重置。
- 第三种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。

TCP 保活的这个机制检测的时间是有点长，我们可以自己在应用层实现一个心跳机制。

可以使用 keepalive_timeout 参数，来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，服务端会启动一个定时器，如果客户端完成一个 HTTP 请求后，在 60 秒内都没有发起新的请求，定时器时间一道，就会触发回调函数来释放该连接。

如果已经建立了连接，但是服务端的进程崩溃会发生什么？

在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手。

###### 1.4 Socket 编程

针对 TCP 应该如何 Socket 编程？

- 服务端和客户端初始化 socket，得到文件描述符
- 服务端调用 bind，将绑定在 IP 地址和端口
- 服务端调用 listen，进行监听
- 服务端调用 accept，等待客户端连接
- 客户端调用 connect，向服务器端的地址和端口发起连接请求
- 服务端 accept 返回用于传输的 socket 的文件描述符
- 客户端调用 write 写入数据；服务端调用 read 读取数据
- 客户端断开连接时，会调用 close，那么服务端 read 读取数据的时候，就会读取到了 EOF，待处理完数据后，服务端调用 close，表示连接关闭

服务端调用 accept 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作监听 socket，一个叫作已完成连接 socket。成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。

accept 发生在三次握手的哪一步？

客户端连接服务端的流程：

- 客户端的协议栈向服务器端发送了 SYN 包，并告诉服务器端当前发送序列号 client_isn，客户端进入 SYN_SENT 状态；
- 服务器端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 client_isn+1，表示对 SYN 包 client_isn 的确认，同时服务器也发送一个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务器端进入 SYN_RCVD 状态；
- 客户端协议栈收到 ACK 之后，使得应用程序从 connect 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务器端的 SYN 包进行应答，应答数据为 server_isn+1；
- ACK 应答包到达服务器端后，服务器端的 TCP 连接进入 ESTABLISHED 状态，同时服务器端协议栈使得 accept 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。

从上面的描述过程，我们可以得知客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。

客户端调用 close 了，连接是断开的流程是什么？

- 客户端调用 close，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态；
- 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，应用程序可以通过 read 调用来感知这个 FIN 包。这个 EOF 会被放在已排队等候的其他已接收的数据之后，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；
- 接着，当处理完数据后，自然就会读到 EOF，于是也调用 close 关闭它的套接字，这会使得服务端发出一个 FIN 包，之后处于 LAST_ACK 状态；
- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；
- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；
- 客户端经过 2MSL 时间之后，也进入 CLOSE 状态；

##### 2. TCP 重传、滑动窗口、流量控制、拥塞控制

###### 2.1 重传机制

1. 超时重传

在发送数据时，设定一个定时器，当超过指定的事件后，没有收到对方的 ACK 确认应答报文，就会重发该数据。TCP 会在以下两种情况发生超时重传：

- 数据包丢失
- 确认应答丢失

超时时间应该设置为多少呢

RTT ( Round-Trip Time 往返时延 ) 指的是数据发送时刻到接收到确认的时刻差值，也就是包的往返时间。超时重传时间是以 RTO ( Retransmission Timeout 超时重传时间 ) 表示。假设在重传的情况下，RTO 设置的较长，重发就慢，丢了之后过了很久才重发，没有效率，性能差，如果设置的较短，会导致可能没有丢失就会重发，增加网络的堵塞，导致更多超时。

所以超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。

如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。

2. 快速重传

另一种快速重传 ( Fast Retransmit ) 机制，它不以时间为驱动，而是以数据驱动重传。快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题，就是重传的时候，是重传之前的一个，还是重传所有的问题。

3. SACK 方法

SACK ( Selective Acknowledgment 选择性确认 ) 这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。

4. Duplicate SACK

Duplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。可见，D-SACK 有以下几个好处：

- 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了
- 可以知道是不是「发送方」的数据包被网络延迟了
- 可以知道网络中是不是把「发送方」的数据包给复制了

###### 2.2 滑动窗口

TCP 引入了窗口概念，避免在数据包的往返时间越长通信效率越低的问题，有了窗口可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。

假设窗口大小为 3 个 TCP 段，那么发送方就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下一个确认应答进行确认」。假设发送序列号为 500 ~ 599 的报文，ACK 600 丢失了，客户端也不会进行数据重发，可以通过下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据接收方都接收到了，这个模式叫做累计确认或者累计应答。

窗口大小由哪一方决定？

TCP 头部里有一个字段叫 Window，也就是窗口大小。这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。所以，通常窗口的大小是由接收方的窗口大小来决定的。发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。

程序是如何表示发送方的四个部分的？

TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针 ( 指特定的序列号 )，一个是相对指针 ( 需要做偏移 )。

- SND.WND：表示发送窗口的大小
- SND.UNA ( Send Unacknoleged )：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号
- SND.NXT：也是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号

接收方的滑动窗口比较简单，只有 RCV.WND 和 RCV.NXT。RCV.WND 表示接收窗口的大小，它会通告给发送方。RCV.NXT 是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号。

接收窗口和发送窗口的大小是相等的吗？

并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。

###### 2.3 流量控制

发送方不能无脑的发数据给接收方，要考虑接收方处理能力。如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。

发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。

为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况。

如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。

接收方向发送方通知窗口大小是通过 ACK 来报告的。如果发生窗口关闭，接收方处理完数据后，会向发送发通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那么就出现了死锁现象，发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据。

TCP 是如何解决窗口关闭，潜在的死锁现象？

为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。

如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。解决糊涂窗口综合症，让接收方不通告小窗口给发送方或是让发送方避免发送小数据。

###### 2.4 拥塞控制

拥塞控制目的就是避免「发送方」的数据填满整个网络。为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。

拥塞窗口 cwnd 是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是 swnd = min ( cwnd, rwnd )，也就是拥塞窗口和接收窗口中的最小值。只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了拥塞。

拥塞控制主要是四个算法：

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，慢启动的算法是当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。假定拥塞窗口 cwnd 和发送窗口 swnd 相等，

- 连接建立完成后，一开始初始化 cwnd = 1，表示可以传一个 MSS 大小的数据。
- 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个。
- 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个。
- 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。

可以看出慢启动算法，发包的个数是指数性的增长。但慢启动有一个门限 ssthresh ( slow start threshold ) 状态变量。当 cwnd < ssthresh 时，使用慢启动算法，当 cwnd >= ssthresh 时，就会使用「拥塞避免算法」。

一般来说 ssthresh 的大小是 65535 字节，进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1 / cwnd。假定 ssthresh 为 8，

- 当 8 个 ACK 应答确认到来时，每个确认增加 1 / 8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 MSS 大小的数据，变成了线性增长。

所以拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。当触发了重传机制，也就进入了「拥塞发生算法」。重传机制有超时重传和快速重传两种，

- 当发生超时重传。ssthresh 设为 cwnd / 2，cwnd 重置为 1 ( 是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1 )。接着，就重新开始慢启动，慢启动是会突然减少数据流的。
- 当发生快速重传，当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传，cwnd = cwnd / 2 ，也就是设置为原来的一半，ssthresh = cwnd，然后进入快速恢复算法。

快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，如果还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。进入快速恢复后，

- 拥塞窗口 cwnd = ssthresh + 3 ( 3 的意思是确认有 3 个数据包被收到了 )
- 重传丢失的数据包
- 如果再收到重复的 ACK，那么 cwnd 增加 1
- 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态

![拥塞控制超时重传恢复。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E8%B6%85%E6%97%B6%E9%87%8D%E4%BC%A0.png)

![拥塞控制快速重传恢复。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/TCP%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E5%BF%AB%E9%80%9F%E9%87%8D%E4%BC%A0.png)

##### 3. TCP 半连接队列和全连接队列

###### 3.1 什么是 TCP 半连接队列和全连接队列？

在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：

- 半连接队列，也称 SYN 队列；
- 全连接队列，也称 accept 队列；

服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN + ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。

不管是半连接队列还是全连接队列，都有最大长度限制，超过限制时，内核会直接丢弃，或返回 RST 包。

##### 4. 如何优化 TCP？

###### 4.1 TCP 三次握手的性能提升

1. 客户端优化

可以根据网络的稳定性和目标服务器的繁忙程度修改 SYN 的重传次数，通过 tcp_syn_retries 控制其重传的次数，调整客户端的三次握手时间上限。比如内网中通讯时，就可以适当调低重试次数，尽快把错误暴露给应用程序。

2. 服务端优化

当半连接队列溢出后，服务端就无法再建立新的连接。SYN 攻击，攻击的是就是这个半连接队列。可以通过 netstat -s  命令给出的统计结果中， 可以得到由于半连接队列已满，引发的失败次数，该输出的数值是累计值，表示共有多少个 TCP 连接因为半连接队列溢出而被丢弃。隔几秒执行几次，如果有上升的趋势，说明当前存在半连接队列溢出的现象。

要想增大半连接队列，不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大 accept 队列。否则，只单纯增大 tcp_max_syn_backlog 是无效的。改变了上述参数后，要重启 Nginx 服务，因为 SYN 半连接队列和 accept 队列都是在 listen( ) 初始化的。

开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接。

当服务端处于 SYN_RCV 状态，即未收到客户端的 ACK，一直在重发 SYN + ACK 报文，当网络繁忙、不稳定时，报文丢失就会变严重，此时应该调大重发次数。反之则可以调小重发次数。修改重发次数的方法是，调整 tcp_synack_retries 参数。

3. 如何绕过三次握手

三次握手建立连接造成的后果就是，HTTP 请求必须在一个 RTT（从客户端到服务器一个往返的时间）后才能发送。在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。

在客户端首次建立连接时的过程：

- 客户端发送 SYN 报文，该报文包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；
- 支持 TCP Fast Open 的服务器生成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；
- 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。

所以，第一次发起 HTTP GET 请求的时候，还是需要正常三次握手。

之后，如果客户端再次向服务器建立连接时的过程：

- 客户端发送 SYN 报文，该报文包含「数据」（对于非 TFO 的普通 TCP 握手过程，SYN 报文中不包含「数据」）以及此前记录的 Cookie；
- 支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「数据」递送至相应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；
- 如果服务器接受了 SYN 报文中的「数据」，服务器可在握手完成之前发送「数据」，这就减少了握手带来的 1 个 RTT 的时间消耗；
- 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报文中发送的「数据」没有被确认，则客户端将重新发送「数据」；
- 此后的 TCP 连接的数据传输过程和非 TFO 的正常情况一致。

所以，之后发起 HTTP GET 请求的时候，可以绕过三次握手，这就减少了握手带来的 1 个 RTT 的时间消耗。开启了 TFO 功能，Cookie 的值是存放到 TCP option 字段里的。

![TCP 三次握手优化策略。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/%E4%BC%98%E5%8C%96%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E7%9A%84%E7%AD%96%E7%95%A5.png)

###### 4.2 TCP 四次挥手的性能提升

四次挥手过程只涉及了两种报文，分别是 FIN 和 ACK：

- FIN 就是结束连接的意思，谁发出 FIN 报文，就表示它将不会再发送任何数据，关闭这一方向上的传输通道；
- ACK 就是确认的意思，用来通知对方：你方的发送通道已经关闭；

四次挥手的过程：

- 当主动方关闭连接时，会发送 FIN 报文，此时发送方的 TCP 连接将从 ESTABLISHED 变成 FIN_WAIT1。
- 当被动方收到 FIN 报文后，内核会自动回复 ACK 报文，连接状态将从 ESTABLISHED 变成 CLOSE_WAIT，表示被动方在等待进程调用 close 函数关闭连接。
- 当主动方收到这个 ACK 后，连接状态由 FIN_WAIT1 变为 FIN_WAIT2，也就是表示主动方的发送通道就关闭了。
- 当被动方进入 CLOSE_WAIT 时，被动方还会继续处理数据，等到进程的 read 函数返回 0 后，应用程序就会调用 close 函数，进而触发内核发送 FIN 报文，此时被动方的连接状态变为 LAST_ACK。
- 当主动方收到这个 FIN 报文后，内核会回复 ACK 报文给被动方，同时主动方的连接状态由 FIN_WAIT2 变为 TIME_WAIT，在 Linux 系统下大约等待 1 分钟后，TIME_WAIT 状态的连接才会彻底关闭。
- 当被动方收到最后的 ACK 报文后，被动方的连接就会关闭。

这里一点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。

1. 主动方的优化

关闭连接的方式通常有两种，分别是 RST 报文关闭和 FIN 报文关闭。

如果进程异常退出了，内核就会发送 RST 报文来关闭，它可以不走四次挥手流程，是一个暴力关闭连接的方式。

安全关闭连接的方式必须通过四次挥手，它由进程调用 close 和 shutdown 函数发起 FIN 报文（shutdown 参数须传入 SHUT_WR 或者 SHUT_RDWR 才会发送 FIN）。

调用了 close 函数意味着完全断开连接，完全断开不仅指无法传输数据，而且也不能发送数据。 此时，调用了 close 函数的一方的连接叫做「孤儿连接」，如果你用 netstat -p 命令，会发现连接对应的进程名为空。

使用 close 函数关闭连接是不优雅的。于是，就出现了一种优雅关闭连接的 shutdown 函数，它可以控制只关闭一个方向的连接。

主动方发送 FIN 报文后，连接就处于 FIN_WAIT1 状态，正常情况下，如果能及时收到被动方的 ACK，则会很快变为 FIN_WAIT2 状态。但是当迟迟收不到对方返回的 ACK 时，连接就会一直处于 FIN_WAIT1 状态。此时，内核会定时重发 FIN 报文，其中重发次数由 tcp_orphan_retries 参数控制。

当进程调用了 close 函数关闭连接，此时连接就会是「孤儿连接」，因为它无法再发送和接收数据。Linux 系统为了防止孤儿连接过多，导致系统资源长时间被占用，就提供了 tcp_max_orphans 参数。如果孤儿连接数量大于它，新增的孤儿连接将不再走四次挥手，而是直接发送 RST 复位报文强制关闭。

当主动方收到 ACK 报文后，会处于 FIN_WAIT2 状态，就表示主动方的发送通道已经关闭，接下来将等待对方发送 FIN 报文，关闭对方的发送通道。

这时，如果连接是用 shutdown 函数关闭的，连接可以一直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收数据。但对于 close 函数关闭的孤儿连接，由于无法再发送和接收数据，所以这个状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长，默认值是 60 秒。

TIME-WAIT 的状态尤其重要，主要是两个原因：

- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；
- 保证「被动关闭连接」的一方，能被正确的关闭；

2. 被动方的优化

当被动方收到 FIN 报文时，内核会自动回复 ACK，同时连接处于 CLOSE_WAIT 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。

内核没有权利替代进程去关闭连接，因为如果主动方是通过 shutdown 关闭连接，那么它就是想在半关闭连接上接收数据或发送数据。因此，Linux 并没有限制 CLOSE_WAIT 状态的持续时间。

当然，大多数应用程序并不使用 shutdown 函数关闭连接。所以，当你用 netstat 命令发现大量 CLOSE_WAIT 状态。就需要排查你的应用程序，因为可能因为应用程序出现了 Bug，read 函数返回 0 时，没有调用 close 函数。

处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文关闭发送通道，同时连接进入 LAST_ACK 状态，等待主动方返回 ACK 来确认连接关闭。

如果迟迟收不到这个 ACK，内核就会重发 FIN 报文，重发次数仍然由 tcp_orphan_retries 参数控制，这与主动方重发 FIN 报文的优化策略一致。

还有一点我们需要注意的，如果被动方迅速调用 close 函数，那么被动方的 ACK 和 FIN 有可能在一个报文中发送，这样看起来，四次挥手会变成三次挥手，这只是一种特殊情况，不用在意。

![TCP 四次挥手优化策略。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/%E4%BC%98%E5%8C%96%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E7%9A%84%E7%AD%96%E7%95%A5.png)

###### 4.3 TCP 传输数据的性能提升

1. 滑动窗口是如何影响传输速度的？

当接收方硬件不如发送方，或者系统繁忙、资源紧张时，是无法瞬间处理过多报文的。于是，报文只能被丢掉，使得网络效率变得非常低，为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是滑动窗口的由来。

接收方根据它的缓冲区，可以计算出后续能够接收多少字节的报文，这个数字叫做接收窗口。当内核接收到报文时，必须用缓冲区存放它们，这样剩余缓冲区空间变小，接收窗口也就变小了；当进程调用 read 函数后，数据被读入了用户空间，内核缓冲区就被清空，这意味着主机可以接收更多的报文，接收窗口就会变大。

因此，接收窗口并不是恒定不变的，接收方会把当前可接收的大小放在 TCP 报文头部中的窗口字段，这样就可以起到窗口大小通知的作用。

发送方的窗口等价于接收方的窗口吗？如果不考虑拥塞控制，发送方的窗口大小「约等于」接收方的窗口大小，因为窗口通知报文在网络传输是存在时延的，所以是约等于的关系。

要使用窗口扩大选项，通讯双方必须在各自的 SYN 报文中发送这个选项：

- 主动建立连接的一方在 SYN 报文中发送这个选项；
- 而被动建立连接的一方只有在收到带窗口扩大选项的 SYN 报文之后才能发送这个选项。

这样看来，只要进程能及时地调用 read 函数读取数据，并且接收缓冲区配置得足够大，那么接收窗口就可以无限地放大，发送方也就无限地提升发送速度。

这是不可能的，因为网络的传输能力是有限的，当发送方依据发送窗口，发送超过网络处理能力的报文时，路由器会直接丢弃这些报文。因此，缓冲区的内存并不是越大越好。

2. 如何确定最大传输速度？

由于发送缓冲区大小决定了发送窗口的上限，而发送窗口又决定了「已发送未确认」的飞行报文的上限。因此，发送缓冲区不能超过「带宽时延积」。

比如最大带宽是 100 MB/s，网络时延（RTT）是 10ms 时，意味着客户端到服务端的网络一共可以存放 100MB/s * 0.01s = 1MB 的字节。

发送缓冲区与带宽时延积的关系：

- 如果发送缓冲区「超过」带宽时延积，超出的部分就没办法有效的网络传输，同时导致网络过载，容易丢包；
- 如果发送缓冲区「小于」带宽时延积，就不能很好的发挥出网络的传输效率。
所以，发送缓冲区的大小最好是往带宽时延积靠近。

3. 怎样调整缓冲区大小？

调节发送缓冲区范围：发送缓冲区是自行调节的，当发送方发送的数据被确认后，并且没有新的数据要发送，就会把发送缓冲区的内存释放掉

调节接收缓冲区范围：接收缓冲区可以根据系统空闲内存的大小来调节接收窗口。接收缓冲区则需要配置 tcp_moderate_rcvbuf 为 1 来开启调节功能

调节 TCP 内存范围：通过 tcp_mem 配置完成。根据当前 tcp_mem 最大内存页面数是 177120，当内存为 (177120 * 4) / 1024K ≈ 692M 时，系统将无法为新的 TCP 连接分配内存，即 TCP 连接将被拒绝。

根据实际场景调节的策略：在高并发服务器中，为了兼顾网速与大量的并发连接，我们应当保证缓冲区的动态调整的最大值达到带宽时延积，而最小值保持默认的 4K 不变即可。而对于内存紧张的服务而言，调低默认值是提高并发的有效手段。同时，如果这是网络 IO 型服务器，那么，调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力。需要注意的是，tcp_wmem 和 tcp_rmem 的单位是字节，而 tcp_mem 的单位是页面大小。而且，千万不要在 socket 上直接设置 SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能。

![数据传输的优化策略。图源小林Coding](https://github.com/ZonzeeLi/Interview/blob/master/picture/%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E7%9A%84%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5.png)

##### 5. 如何理解 TCP 是面向字节流协议？

###### 5.1 如何理解字节流？

每个 UDP 报文就是一个用户消息的边界，这样接收方在接收到 UDP 报文后，读一个 UDP 报文就能读取到完整的用户消息。操作系统在收到 UDP 报文后，会将其插入到队列里，队列里的每一个元素就是一个 UDP 报文。

当用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。至于什么时候真正被发送，取决于发送窗口、拥塞窗口以及当前发送缓冲区的大小等条件。我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议。

当两个消息的某个部分内容被分到同一个 TCP 报文时，就是我们常说的 TCP 粘包问题，这时接收方不知道消息的边界的话，是无法读出有效的消息。要解决这个问题，要交给应用程序。

###### 5.2 如何解决粘包？

粘包的问题出现是因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。

一般有三种方式分包的方式：

- 固定长度的消息：每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。但是这种方式灵活性不高，实际中很少用。
- 特殊字符作为边界：HTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界。如果刚好消息内容里有这个特殊字符，我们要对这个字符转义，避免被接收方当作消息的边界点而解析到无效的数据。
- 自定义消息结构：可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。

##### 6. 为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？

主要原因是为了防止历史报文被下一个相同四元组的连接接收。

TCP 四次挥手中的 TIME_WAIT 状态不是会持续 2 MSL 时长，历史报文不是早就在网络中消失了吗？

假设：

- 客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，而此时服务端的进程重启了，于是就会发送 RST 报文来断开连接。
- 紧接着，客户端又与服务端建立了与上一个连接相同四元组的连接；
- 在新连接建立完成后，上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。

##### 7. SYN 报文什么时候情况下会被丢弃？

SYN 报文被丢弃的两种场景：

- 开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃
- TCP 两个队列满了（半连接队列和全连接队列），造成 SYN 报文被丢弃

针对半连接队列满了，当服务器造成syn攻击，就有可能导致 TCP 半连接队列满了，这时后面来的 syn 包都会被丢弃。但是，如果开启了syncookies 功能，即使半连接队列满了，也不会丢弃 syn 包。

防御 SYN 攻击的方法：

- 增大半连接队列，不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大全连接队列；
- 开启 tcp_syncookies 功能；
- 减少 SYN+ACK 重传次数
					

针对全连接队列满了，在服务端并发处理大量请求时，如果 TCP accpet 队列过小，或者应用程序调用 accept() 不及时，就会造成 accpet 队列满了 ，这时后续的连接就会被丢弃，这样就会出现服务端请求数量上不去的现象。

要解决这个问题，可以：

- 调大 accpet 队列的最大长度，调大的方式是通过调大 backlog 以及 somaxconn 参数。
- 检查系统或者代码为什么调用 accept() 不及时。

##### 8. 已建立连接的TCP，收到SYN会发生什么？

分为两种情况：

1. 客户端的 SYN 报文里的端口号与历史连接不相同

如果客户端恢复后发送的 SYN 报文中的源端口号跟上一次连接的源端口号不一样，此时服务端会认为是新的连接要建立，于是就会通过三次握手来建立新的连接。而旧连接，由于客户端的连接已经被关闭了，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接。如果服务端一直没有发送数据包给客户端，在超过一段时间后， TCP 保活机制就会启动，检测到客户端没有存活后，接着服务端就会释放掉该连接。

2. 客户端的 SYN 报文里的端口号与历史连接相同

处于 establish 状态的服务端如果收到了客户端的 SYN 报文（注意此时的 SYN 报文其实是乱序的，因为 SYN 报文的初始化序列号其实是一个随机数），会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK。
接着，客户端收到这个 Challenge ACK，发现序列号并不是自己期望收到的，于是就会回 RST 报文，服务端收到后，就会释放掉该连接。

如何关闭一个 TCP 连接？

要伪造一个能关闭 TCP 连接的 RST 报文，必须同时满足「四元组相同」和「序列号正好落在对方的滑动窗口内」这两个条件。

我们可以伪造一个四元组相同的 SYN 报文，来拿到“合法”的序列号。如果处于 establish 状态的服务端，收到四元组相同的 SYN 报文后，会回复一个 Challenge ACK，这个 ACK 报文里的「确认号」，正好是服务端下一次想要接收的序列号，说白了，就是可以通过这一步拿到服务端下一次预期接收的序列号。然后用这个确认号作为 RST 报文的序列号，发送给服务端，此时服务端会认为这个 RST 报文里的序列号是合法的，于是就会释放连接！

##### 9. 四次挥手中收到乱序的 FIN 包会如何处理？

可以理解为：

- 主动方 FIN，ACK，seq = x，ack = y （shutdown关闭写反向，进入FIN_WAIT_1状态）
- 被动方 ACK，seq = y，ack = x+1 （进入CLOSE_WAIT状态）
- 被动方 PUSH，ACK，seq = y，ack = x+1，len = 100 （被阻塞）
- 被动方 FIN，seq = y+100，ack = x+1 （收到ACK，进入FIN_WAIT_2状态，乱序）

在 FIN_WAIT_2 状态下，是如何处理收到的乱序到 FIN 报文，然后 TCP 连接又是什么时候才进入到 TIME_WAIT 状态?

在 FIN_WAIT_2 状态时，如果收到乱序的 FIN 报文，那么就被会加入到「乱序队列」，并不会进入到 TIME_WAIT 状态。等再次收到前面被网络延迟的数据包时，会判断乱序队列有没有数据，然后会检测乱序队列中是否有可用的数据，如果能在乱序队列中找到与当前报文的序列号保持的顺序的报文，就会看该报文是否有 FIN 标志，如果发现有 FIN 标志，这时才会进入 TIME_WAIT 状态。

##### 10. 在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？

如果处于 TIME_WAIT 状态的连接收到「合法的 SYN 」后，就会重用此四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。

如果处于 TIME_WAIT 状态的连接收到「非法的 SYN 」后，就会再回复一个第四次挥手的 ACK 报文，客户端（这里的主动方和被动方交换了）收到后，发现并不是自己期望收到确认号（ack num），就回 RST 报文给服务端。

##### 11. TCP 连接，一端断电和进程崩溃有什么区别？

鹅厂提问：一个 TCP 连接，没有打开 keepalive 选项，没有数据交互，现在一端突然掉电和一端的进程 crash，这两种情况有什么区别？

TCP 的 keepalive 其实是 TCP 的保活机制，在一个时间段内，没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一段时间发送一个探测报文，如果该报文没有得到响应，那么认为当前 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

如果客户端的「主机崩溃」了，服务端是无法感知到的，在加上服务端没有开启 TCP keepalive，又没有数据交互的情况下，服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态，直到服务端重启进程。

如果是「进程崩溃」的情况，发现在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手。

在「有数据传输」的情况下，有两种情况，客户端主机宕机，又迅速重启和一直没有重启，这两种情况分别会发生什么？

在客户端主机宕机后，服务端向客户端发送的报文会得不到任何的响应，在一定时长后，服务端就会触发超时重传机制，重传未得到响应的报文。客户端主机重启完成后，客户端的内核就会接收重传的报文，然后根据报文的信息传递给对应的进程，如果客户端主机上没有进程监听该 TCP 报文的目标端口号，那么客户端内核就会回复 RST 报文，重置该 TCP 连接。如果客户端主机上有进程监听该 TCP 报文的目标端口号，由于客户端主机重启后，之前的 TCP 连接的数据结构已经丢失了，客户端内核里协议栈会发现找不到该 TCP 连接的 socket 结构体，于是就会回复 RST 报文，重置该 TCP 连接。

只要有一方重启完成后，收到之前 TCP 连接的报文，都会回复 RST 报文，以断开连接。

如果是客户端一直没有重启，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，一般就是 ETIMEOUT 状态码。

##### 12. 拔掉网线后， 原本的 TCP 连接还存在吗？

实际上，TCP 连接在 Linux 内核中是一个名为 struct socket 的结构体，该结构体的内容包含 TCP 连接的状态等信息。当拔掉网线的时候，操作系统并不会变更该结构体的任何内容，所以 TCP 连接的状态也不会发生改变。

在客户端拔掉网线后，服务端向客户端发送的数据报文会得不到任何的响应，在等待一定时长后，服务端就会触发超时重传机制，重传未得到响应的数据报文。

如果在服务端重传报文的过程中，客户端刚好把网线插回去了，由于拔掉网线并不会改变客户端的 TCP 连接状态，并且还是处于 ESTABLISHED 状态，所以这时客户端是可以正常接收服务端发来的数据报文的，然后客户端就会回 ACK 响应报文。

此时，客户端和服务端的 TCP 连接依然存在的，就感觉什么事情都没有发生。

但是，如果如果在服务端重传报文的过程中，客户端一直没有将网线插回去，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。

而等客户端插回网线后，如果客户端向服务端发送了数据，由于服务端已经没有与客户端相同四元祖的 TCP 连接了，因此服务端内核就会回复 RST 报文，客户端收到后就会释放该 TCP 连接。

此时，客户端和服务端的 TCP 连接都已经断开了。

针对拔掉网线后，没有数据传输的场景，还得看是否开启了 TCP keepalive 机制 （TCP 保活机制）。

如果没有开启 TCP keepalive 机制，在客户端拔掉网线后，并且双方都没有进行数据传输，那么客户端和服务端的 TCP 连接将会一直保持存在。

而如果开启了 TCP keepalive 机制，在客户端拔掉网线后，即使双方都没有进行数据传输，在持续一段时间后，TCP 就会发送探测报文：

- 如果对端是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。
- 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。

##### 13. tcp_tw_reuse 为什么默认是关闭的？

tcp_tw_reuse 的作用是让客户端快速复用处于 TIME_WAIT 状态的端口，相当于跳过了 TIME_WAIT 状态，这可能会出现这样的两个问题：

- 历史 RST 报文可能会终止后面相同四元组的连接，因为 PAWS 检查到即使 RST 是过期的，也不会丢弃。
- 如果第四次挥手的 ACK 报文丢失了，有可能被动关闭连接的一方不能被正常的关闭;

##### 14. HTTPS 中 TLS 和 TCP 能同时握手吗？

一般情况下，不管 TLS 握手次数如何，都得先经过 TCP 三次握手后才能进行，因为 HTTPS 都是基于 TCP 传输协议实现的，得先建立完可靠的 TCP 连接才能做 TLS 握手的事情。

但如果同时满足以下两个条件就可以：

- 客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3；
- 客户端和服务端已经完成过一次通信。

##### 15. TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？

首先，二者是两个完全不一样的两个层面的东西：

- HTTP 的 Keep-Alive，是由应用层（用户态） 实现的，称为 HTTP 长连接；
- TCP 的 Keepalive，是由 TCP 层（内核态） 实现的，称为 TCP 保活机制；

HTTP 协议采用的是「请求-应答」的模式，也就是客户端发起了请求，服务端才会返回响应，一来一回这样子。如果每次请求都要经历这样的过程：建立 TCP -> 请求资源 -> 响应资源 -> 释放连接，那么此方式就是 HTTP 短连接，HTTP 的 Keep-Alive 可以使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 HTTP 长连接。HTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

TCP 的 Keepalive 这东西其实就是 TCP 的保活机制，如果两端的 TCP 连接一直没有数据交互，达到了触发 TCP 保活机制的条件，那么内核里的 TCP 协议栈就会发送探测报文。

- 如果对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 TCP 保活时间会被重置，等待下一个 TCP 保活时间的到来。
- 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。

##### 16. TCP 协议有什么缺陷？

- 升级 TCP 的工作很困难
- TCP 建立连接的延迟
- TCP 存在队头阻塞问题
- 网络迁移需要重新建立 TCP 连接

##### 17. 如何基于 UDP 协议实现可靠传输？

###### 17.1 QUIC 是如何实现可靠传输的？

Packet Header 首次建立连接时和日常传输数据时使用的 Header 是不同的。Packet Header 细分这两种：

- Long Packet Header 用于首次建立连接。
- Short Packet Header 用于日常传输数据。

QUIC 也是需要三次握手来建立连接的，主要目的是为了协商连接 ID。协商出连接 ID 后，后续传输时，双方只需要固定住连接 ID，从而实现连接迁移功能。所以，你可以看到日常传输数据的 Short Packet Header 不需要在传输 Source Connection ID 字段了，只需要传输 Destination Connection ID。

Short Packet Header 中的 Packet Number 是每个报文独一无二的编号，它是严格递增的，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。这样子会使 RTT 计算更精准

还有一个好处，QUIC 使用的 Packet Number 单调递增的设计，可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动。

一个 Packet 报文中可以存放多个 QUIC Frame。每一个 Frame 都有明确的类型，针对类型的不同，功能也不同，自然格式也不同。以 Stream 类型为例：

- Stream ID 作用：多个并发传输的 HTTP 消息，通过不同的 Stream ID 加以区别，类似于 HTTP2 的 Stream ID
- Offset 作用：类似于 TCP 协议中的 Seq 序号，保证数据的顺序性和可靠性
- Length 作用：指明了 Frame 数据的长度

通过 Stream ID + Offset 字段信息实现数据的有序性，比较两个数据包的 Stream ID 与 Stream Offset ，如果都是一致，就说明这两个数据包的内容一致。

###### 17.2 QUIC 是如何解决 TCP 队头阻塞问题的？

QUIC 也借鉴 HTTP /2 里的 Stream 的概念，在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (Stream)。但是 HTTP /2 多个 Stream 请求都是在一条 TCP 连接上传输，这意味着多个 Stream 共用同一个 TCP 滑动窗口，那么当发生数据丢失，滑动窗口是无法往前移动的，此时就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。TCP 队头阻塞的问题要从两个角度看，一个是发送窗口的队头阻塞，另外一个是接收窗口的队头阻塞。

但是 QUIC 给每一个 Stream 都分配了一个独立的滑动窗口，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口。

###### 17.3 QUIC 是如何做流量控制的？

TCP 流量控制是通过让「接收方」告诉「发送方」，它（接收方）的接收窗口有多大，从而让「发送方」根据「接收方」的实际接收能力控制发送的数据量。

QUIC 实现流量控制的方式：

- 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。
- 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。

QUIC 实现了两种级别的流量控制，分别为 Stream 和 Connection 两种级别：

- Stream 级别的流量控制：Stream 可以认为就是一条 HTTP 请求，每个 Stream 都有独立的滑动窗口，所以每个 Stream 都可以做流量控制，防止单个 Stream 消耗连接（Connection）的全部接收缓冲。
- Connection 流量控制：限制连接中所有 Stream 相加起来的总字节数，防止发送方超过连接的缓冲容量。

QUIC 的流量控制和 TCP 有点区别，TCP 的接收窗口只有在前面所有的 Segment 都接收的情况下才会移动左边界，当在前面还有字节未接收但收到后面字节的情况下，窗口也不会移动，
而 QUIC 的接收窗口的左边界滑动条件取决于接收到的最大偏移字节数。当已收到且被读取的数据超过最大接收窗口的一半后，最大接收窗口向右移动，接收窗口的右边界也向右扩展，同时给对端发送「窗口更新帧」，当发送方收到接收方的窗口更新帧后，发送窗口的右边界也会往右扩展，以此达到窗口滑动的效果。

###### 17.4 QUIC 对拥塞控制改进

QUIC 是处于应用层的，应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。这是一个飞跃，因为传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。而内核和操作系统的部署成本非常高，升级周期很长，所以 TCP 拥塞控制算法迭代速度是很慢的。而 QUIC 可以随浏览器更新，QUIC 的拥塞控制算法就可以有较快的迭代速度。

###### 17.5 QUIC 更快的连接建立

HTTP /3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。

但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。

###### 17.6 QUIC 是如何迁移连接的？

QUIC 协议没有用四元组的方式来“绑定”连接，而是通过连接 ID来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。

#### IP

##### 1. IP 基本知识

IP 在 TCP/IP 参考模型中处于第三层，也就是网络层。

网络层的主要作用是：实现主机与主机之间的通信，也叫点对点（end to end）通信。

网络层与数据链路层有什么关系？

IP 的作用是主机之间通信用的，而 MAC 的作用则是实现「直连」的两个设备之间通信，而 IP 则负责在「没有直连」的两个网络之间进行通信传输。

##### 2. IP 地址的基础知识

###### 2.1 IP 地址的分类

IP 地址分类成了 5 种类型，分别是 A 类、B 类、C 类、D 类、E 类。

![IP 地址分类。图源小林Coding]()

黄色部分为分类号，用以区分 IP 地址类别。

什么是 A、B、C 类地址？

其中对于 A、B、C 类主要分为两个部分，分别是网络号和主机号。最大主机个数，就是要看主机号的位数，而要减去两个特殊的 IP，分别是主机号全为 1 和 0，主机号全为 1 指定某个网络下的所有主机，用于广播，主机号全为 0 指定某个网络，在分配时要去掉这两种情况。

广播地址用于在同一个链路中相互连接的主机之间发送数据包。广播地址可以分为本地广播和直接广播两种。

- 在本网络内广播的叫做本地广播。例如网络地址为 192.168.0.0/24 的情况下，广播地址是 192.168.0.255 。因为这个广播地址的 IP 包会被路由器屏蔽，所以不会到达 192.168.0.0/24 以外的其他链路上。
- 在不同网络之间的广播叫做直接广播。例如网络地址为 192.168.0.0/24 的主机向 192.168.1.255/24 的目标地址发送 IP 包。收到这个包的路由器，将数据转发给 192.168.1.0/24，从而使得所有 192.168.1.1~192.168.1.254 的主机都能收到这个包（由于直接广播有一定的安全问题，多数情况下会在路由器上设置为不转发） 。

而 D 类和 E 类地址是没有主机号的，所以不可用于主机 IP，D 类常被用于多播，E 类是预留的分类，暂时未使用。多播用于将包发送给特定组内的所有主机。多播使用的 D 类地址，其前四位是 1110 就表示是多播地址，而剩下的 28 位是多播的组编号。从 224.0.0.0 ~ 239.255.255.255 都是多播的可用范围，其划分为以下三类：

- 224.0.0.0 ~ 224.0.0.255 为预留的组播地址，只能在局域网中，路由器是不会进行转发的。
- 224.0.1.0 ~ 238.255.255.255 为用户可用的组播地址，可以用于 Internet 上。
- 239.0.0.0 ~ 239.255.255.255 为本地管理组播地址，可供内部网在内部使用，仅在特定的本地范围内有效。

IP 分类的优点：简单明了、选路（基于网络地址）简单。

IP 分类的缺点：

- 同一网络下没有地址层次，比如一个公司里用了 B 类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种 IP 分类是没有地址层次划分的功能，所以这就缺少地址的灵活性。
- A、B、C类有个尴尬处境，就是不能很好的与现实网络匹配。

这两个缺点，都可以在 CIDR 无分类地址解决。

###### 2.2 无分类地址 CIDR

因为 IP 分类有很多的缺点，所以后面提出了无分类地址 CIDR，32 比特的 IP 地址被划分为两部分，前面是网络号，后面是主机号。

表示的形式为 a.b.c.d/x，其中 /x 表示前 x 位属于网络号，x 的范围是 0~32，这就使得 IP 地址更加具有灵活性。

另一种划分网络号与主机号形式，就是子网掩码，掩码的意思就是掩盖掉主机号，剩余的就是网络号。将子网掩码和 IP 地址按位计算 AND，就可得到网络号。

子网掩码的另一个作用就是划分子网，子网划分实际上是将主机地址分为两个部分：子网网络地址和子网主机地址。即：

- 未做子网划分的 IP 地址：网络地址＋主机地址
- 做子网划分后的 IP 地址：网络地址＋（子网网络地址＋子网主机地址）

###### 2.3 公有 IP 地址与私有 IP 地址

在 A、B、C 分类地址，实际上有分公有 IP 地址和私有 IP 地址。平时我们办公室、家里、学校用的 IP 地址，一般都是私有 IP 地址。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，两个学校的某个私有 IP 地址可能是一样的。

私有 IP 地址通常是内部的 IT 人员管理，公有 IP 地址是由 ICANN 组织管理，中文叫「互联网名称与数字地址分配机构」。

###### 2.4 IP 地址与路由控制

IP地址的网络地址这一部分是用于进行路由控制。路由控制表中记录着网络地址与下一步应该发送至路由器的地址。在主机和路由器上都会有各自的路由器控制表。

在发送 IP 包时，首先要确定 IP 包首部中的目标地址，再从路由控制表中找到与该地址具有相同网络地址的记录，根据该记录将 IP 包转发给相应的下一个路由器。如果路由控制表中存在多条相同网络地址的记录，就选择相同位数最多的网络地址，也就是最长匹配。

环回地址是在同一台计算机上的程序之间进行网络通信时所使用的一个默认地址。

计算机使用一个特殊的 IP 地址 127.0.0.1 作为环回地址。与该地址具有相同意义的是一个叫做 localhost 的主机名。使用这个 IP 或主机名时，数据包不会流向网络。

###### 2.5 IP 分片与重组

每种数据链路的最大传输单元 MTU 都是不相同的，是因为每个不同类型的数据链路的使用目的不同。使用目的不同，可承载的 MTU 也就不同。其中，我们最常见数据链路是以太网，它的 MTU 是 1500 字节。当 IP 数据包大小大于 MTU 时， IP 数据包就会被分片。经过分片之后的 IP 数据报在被重组的时候，只能由目标主机进行，路由器是不会进行重组的。

在分片传输中，一旦某个分片丢失，则会造成整个 IP 数据报作废，所以 TCP 引入了 MSS 也就是在 TCP 层进行分片不由 IP 层分片，那么对于 UDP 我们尽量不要发送一个大于 MTU 的数据报文。

###### 2.6 IPv6 基本认识

IPv4 的地址是 32 位的，大约可以提供 42 亿个地址，但是早在 2011 年 IPv4 地址就已经被分配完了。但是 IPv6 的地址是 128 位的，而且还有更好的安全性和扩展性，说简单点就是 IPv6 相比于 IPv4 能带来更好的网络体验。

IPv6 的优点：

- IPv6 可自动配置，即使没有 DHCP 服务器也可以实现自动分配IP地址，真是便捷到即插即用啊。
- IPv6 包头包首部长度采用固定的值 40 字节，去掉了包头校验和，简化了首部结构，减轻了路由器负荷，大大提高了传输的性能。
- IPv6 有应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能，大大提升了安全性。

IPv6 地址的标识方法为每 16 位为一组，每组用 : 隔开，如果出现连续的 0 时还可以将这些 0 省略，并用两个冒号 : : 隔开。但是，一个 IP 地址中只允许出现一次两个连续的冒号。

IPv6 地址的结构：

- 单播地址，用于一对一的通信
- 组播地址，用于一对多的通信
- 任播地址，用于通信最近的节点，最近的节点是由路由协议决定
- 没有广播地址

![IPv6地址类型。图源小林Coding]()

IPv6 对于一对一通信的 IPv6 地址，主要划分了三类单播地址，每类地址的有效范围都不同。

- 在同一链路单播通信，不经过路由器，可以使用链路本地单播地址，IPv4 没有此类型
- 在内网里单播通信，可以使用唯一本地地址，相当于 IPv4 的私有 IP
- 在互联网通信，可以使用全局单播地址，相当于 IPv4 的公有 IP

IPv6 相比 IPv4 的首部改进：

- 取消了首部校验和字段。 因为在数据链路层和传输层都会校验，因此 IPv6 直接取消了 IP 的校验。
- 取消了分片/重新组装相关字段。 分片与重组是耗时的过程，IPv6 不允许在中间路由器进行分片与重组，这种操作只能在源与目标主机，这将大大提高了路由器转发的速度。
- 取消选项字段。 选项字段不再是标准 IP 首部的一部分了，但它并没有消失，而是可能出现在 IPv6 首部中的「下一个首部」指出的位置上。删除该选项字段使的 IPv6 的首部成为固定长度的 40 字节。

##### 3. IP 协议相关技术

###### 3.1 DNS

域名的层级关系类似一个树状结构：

- 根 DNS 服务器
- 顶级域 DNS 服务器（com）
- 权威 DNS 服务器（server.com）

DNS 域名解析的工作流程就是现在本地域名解析文件 hosts 中查询，如果没有就会从根依次去询问。

###### 3.2 ARP

在传输一个 IP 数据报的时候，确定了源 IP 地址和目标 IP 地址后，就会通过主机「路由表」确定 IP 数据包下一跳。然而，网络层的下一层是数据链路层，所以我们还要知道「下一跳」的 MAC 地址。由于主机的路由表中可以找到下一跳的 IP 地址，所以可以通过 ARP 协议，求得下一跳的 MAC 地址。

ARP 是借助 ARP 请求与 ARP 响应两种类型的包确定 MAC 地址的。

- 主机会通过广播发送 ARP 请求，这个包中包含了想要知道的 MAC 地址的主机 IP 地址。
- 当同个链路中的所有设备收到 ARP 请求时，会去拆开 ARP 请求包里的内容，如果 ARP 请求包中的目标 IP 地址与自己的 IP 地址一致，那么这个设备就将自己的 MAC 地址塞入 ARP 响应包返回给主机。

操作系统通常会把第一次通过 ARP 获取的 MAC 地址缓存起来，以便下次直接从缓存中找到对应 IP 地址的 MAC 地址。不过，MAC 地址的缓存是有一定期限的，超过这个期限，缓存的内容将被清除。

ARP 协议是已知 IP 地址求 MAC 地址，那 RARP 协议正好相反，它是已知 MAC 地址求 IP 地址。

这需要架设一台 RARP 服务器，在这个服务器上注册设备的 MAC 地址及其 IP 地址。然后再将这个设备接入到网络，接着：

- 该设备会发送一条「我的 MAC 地址是XXXX，请告诉我，我的IP地址应该是什么」的请求信息。
- RARP 服务器接到这个消息后返回「MAC地址为 XXXX 的设备，IP地址为 XXXX」的信息给这个设备。

###### 3.3 DHCP

我们的电脑通常都是通过 DHCP 动态获取 IP 地址，大大省去了配 IP 信息繁琐的过程。DHCP 客户端进程监听的是 68 端口号，DHCP 服务端进程监听的是 67 端口号。

- 客户端首先发起 DHCP 发现报文（DHCP DISCOVER） 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP 广播通信，其使用的广播目的地址是 255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该 IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。
- DHCP 服务器收到 DHCP 发现报文时，用 DHCP 提供报文（DHCP OFFER） 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 IP 地址租用期。
- 客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 DHCP 请求报文（DHCP REQUEST进行响应，回显配置的参数。
- 最后，服务端用 DHCP ACK 报文对 DHCP 请求报文进行响应，应答所要求的参数。

一旦客户端收到 DHCP ACK 后，交互便完成了，并且客户端能够在租用期内使用 DHCP 服务器分配的 IP 地址。

如果租约的 DHCP IP 地址快期后，客户端会向服务器发送 DHCP 请求报文：

- 服务器如果同意继续租用，则用 DHCP ACK 报文进行应答，客户端就会延长租期。
- 服务器如果不同意继续租用，则用 DHCP NACK 报文，客户端就要停止使用租约的 IP 地址。

DHCP 交互中，全程都是使用 UDP 广播通信。但是如果是广播通信，DHCP 服务器和客户端不在同一个局域网内，路由器不会转发广播包，就需要每一个网络配一个 DHCP 服务器，而如果有了 DHCP 中继代理，对不同网段的 IP 地址分配也可以由一个 DHCP 服务器统一进行管理。

- DHCP 客户端会向 DHCP 中继代理发送 DHCP 请求包，而 DHCP 中继代理在收到这个广播包以后，再以单播的形式发给 DHCP 服务器。
- 服务器端收到该包以后再向 DHCP 中继代理返回应答，并由 DHCP 中继代理将此包广播给 DHCP 客户端 。

###### 3.4 NAT

网络地址转换 NAT 的方法，可以缓解了 IPv4 地址耗尽的问题。把私有 IP 地址转换成公有 IP 地址。普通的 NAT 转换没什么意义，由于绝大多数的网络应用都是使用传输层协议 TCP 或 UDP 来传输数据的。因此，可以把 IP 地址 + 端口号一起进行转换。这样，就用一个全球 IP 地址就可以了，这种转换技术就叫网络地址与端口转换 NAPT。假设有两个客户端 192.168.1.10 和 192.168.1.11 同时与服务器 183.232.231.172 进行通信，并且这两个客户端的本地端口都是 1025。此时，两个私有 IP 地址都转换 IP 地址为公有地址 120.229.175.121，但是以不同的端口号作为区分。于是，生成一个 NAPT 路由器的转换表，就可以正确地转换地址跟端口的组合，令客户端 A、B 能同时与服务器之间进行通信。

这种转换表在 NAT 路由器上自动生成。例如，在 TCP 的情况下，建立 TCP 连接首次握手时的 SYN 包一经发出，就会生成这个表。而后又随着收到关闭连接时发出 FIN 包的确认应答从表中被删除。

由于 NAT/NAPT 都依赖于自己的转换表，因此会有以下的问题：

- 外部无法主动与 NAT 内部服务器建立连接，因为 NAPT 转换表没有转换记录。
- 转换表的生成与转换操作都会产生性能开销。
- 通信过程中，如果 NAT 路由器重启了，所有的 TCP 连接都将被重置。

解决的方法有两种：

- 使用 IPv6。IPv6的可用范围巨大，每台设备都可以配置一个公网 IP。
- NAT 穿透技术。它能够让网络应用程序主动发现自己位于 NAT 设备之后，并且会主动获得 NAT 设备的公有 IP，并为自己建立端口映射条目，注意这些都是 NAT 设备后的应用程序自动完成的。也就是说，在 NAT 穿透技术中，NAT设备后的应用程序处于主动地位，它已经明确地知道 NAT 设备要修改它外发的数据包，于是它主动配合 NAT 设备的操作，主动地建立好映射，这样就不像以前由 NAT 设备来建立映射了。也就是客户端主动从 NAT 设备获取公有 IP 地址，然后自己建立端口映射条目，然后用这个条目对外通信，就不需要 NAT 设备来进行转换了。

###### 3.5 ICMP

ICMP 主要的功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。在 IP 通信中如果某个 IP 包因为某种原因未能达到目标地址，那么这个具体的原因将由 ICMP 负责通知。

ICMP 大致可以分为两大类，查询报文类型和差错报文类型。

###### 3.6 IGMP

组播地址，也就是 D 类地址，既然是组播，那就说明是只有一组的主机能收到数据包，不在一组的主机不能收到数组包，怎么管理是否是在一组呢？那么，就需要 IGMP 协议了。IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间。

- IGMP 报文向路由器申请加入和退出组播组，默认情况下路由器是不会转发组播包到连接中的主机，除非主机通过 IGMP 加入到组播组，主机申请加入到组播组时，路由器就会记录 IGMP 路由器表，路由器后续就会转发组播包到对应的主机了。
- IGMP 报文采用 IP 封装，IP 头部的协议号为 2，而且 TTL 字段值通常为 1，因为 IGMP 是工作在主机与连接的路由器之间。

常规查询与响应工作机制

- 路由器会周期性发送目的地址为 224.0.0.1（表示同一网段内所有主机和路由器） IGMP 常规查询报文。
- 主机 a 和 主机 b 收到这个查询，随后会启动「报告延迟计时器」，计时器的时间是随机的，通常是 0~10 秒，计时器超时后主机就会发送 IGMP 成员关系报告报文（源 IP 地址为自己主机的 IP 地址，目的 IP 地址为组播地址）。如果在定时器超时之前，收到同一个组内的其他主机发送的成员关系报告报文，则自己不再发送，这样可以减少网络中多余的 IGMP 报文数量。
- 路由器收到主机的成员关系报文后，就会在 IGMP 路由表中加入该组播组，后续网络中一旦该组播地址的数据到达路由器，它会把数据包转发出去。

离开组播组工作机制

有两种情况，第一种网段中仍有该组播组：

- 主机 a 要离开组 224.1.1.1，发送 IGMPv2 离组报文，报文的目的地址是 224.0.0.2（表示发向网段内的所有路由器）
- 路由器 收到该报文后，以 1 秒为间隔连续发送 IGMP 特定组查询报文（共计发送 2 个），以便确认该网络是否还有 224.1.1.1 组的其他成员。
- 主机 3 仍然是组 224.1.1.1 的成员，因此它立即响应这个特定组查询。路由器知道该网络中仍然存在该组播组的成员，于是继续向该网络转发 224.1.1.1 的组播数据包。

第二种情况，网段中没有该组播组：

- 主机 a 要离开组播组 224.1.1.1，发送 IGMP 离组报文。
- 路由器收到该报文后，以 1 秒为间隔连续发送 IGMP 特定组查询报文（共计发送 2 个）。此时在该网段内，组 224.1.1.1 已经没有其他成员了，因此没有主机响应这个查询。
- 一定时间后，路由器认为该网段中已经没有 224.1.1.1 组播组成员了，将不会再向这个网段转发该组播地址的数据包。

##### 4. ping 的工作原理

###### 4.1 ICMP 协议

ICMP 全称是 Internet Control Message Protocol，也就是互联网控制报文协议。

ICMP 主要的功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。在 IP 通信中如果某个 IP 包因为某种原因未能达到目标地址，那么这个具体的原因将由 ICMP 负责通知。

ICMP 报文是封装在 IP 包里面，它工作在网络层，是 IP 协议的助手。

ICMP 包头的类型字段，大致可以分为两大类：查询报文类型和差错报文类型。

![ICMP 报文类型。图源小林Coding]()

###### 4.2 查询报文类型

回送消息用于进行通信的主机或路由器之间，判断所发送的数据包是否已经成功到达对端的一种消息，ping 命令就是利用这个消息实现的。可以向对端主机发送回送请求的消息（ICMP Echo Request Message，类型 8），也可以接收对端主机发回来的回送应答消息（ICMP Echo Reply Message，类型 0）。

###### 4.3 差错报文类型

目标不可达消息（Destination Unreachable Message），类型为 3。IP 路由器无法将 IP 数据包发送给目标地址时，会给发送端主机返回一个目标不可达的 ICMP 消息，并在这个消息中显示不可达的具体原因，原因记录在 ICMP 包头的代码字段。由此，根据 ICMP 不可达的具体消息，发送端主机也就可以了解此次发送不可达的具体原因。有6 种常见的目标不可达类型的代码：

- 网络不可达代码为 
- 主机不可达代码为 1
- 协议不可达代码为 2
- 端口不可达代码为 3
- 需要进行分片但设置了不分片位代码为 4

原点抑制消息（ICMP Source Quench Message），类型 4。在使用低速广域线路的情况下，连接 WAN 的路由器可能会遇到网络拥堵的问题。ICMP 原点抑制消息的目的就是为了缓和这种拥堵情况。当路由器向低速线路发送数据时，其发送队列的缓存变为零而无法发送出去时，可以向 IP 包的源地址发送一个 ICMP 原点抑制消息。收到这个消息的主机借此了解在整个线路的某一处发生了拥堵的情况，从而增大 IP 包的传输间隔，减少网络拥堵的情况。

重定向消息（ICMP Redirect Message），类型 5。如果路由器发现发送端主机使用了「不是最优」的路径发送数据，那么它会返回一个 ICMP 重定向消息给这个主机。在这个消息中包含了最合适的路由信息和源数据。这主要发生在路由器持有更好的路由信息的情况下。路由器会通过这样的 ICMP 消息告知发送端，让它下次发给另外一个路由器。

超时消息（ICMP Time Exceeded Message），类型 11。IP 包中有一个字段叫做 TTL（Time To Live，生存周期），它的值随着每经过一次路由器就会减 1，直到减到 0 时该 IP 包会被丢弃。此时，路由器将会发送一个 ICMP 超时消息给发送端主机，并通知该包已被丢弃。设置 IP 包生存周期的主要目的，是为了在路由控制遇到问题发生循环状况时，避免 IP 包无休止地在网络上被转发。

###### 4.4 ping —— 查询报文类型的使用

ping 命令执行的时候，源主机首先会构建一个 ICMP 回送请求消息数据包。ICMP 数据包内包含多个字段，最重要的是两个：

- 第一个是类型，对于回送请求消息而言该字段为 8；
- 另外一个是序号，主要用于区分连续 ping 的时候发出的多个数据包。

每发出一个请求数据包，序号会自动加 1。为了能够计算往返时间 RTT，它会在报文的数据部分插入发送时间。

然后，主机 A 由 ICMP 协议将这个数据包连同地址 192.168.1.2 一起交给 IP 层。IP 层将以 192.168.1.2 作为目的地址，本机 IP 地址作为源地址，协议字段设置为 1 表示是 ICMP 协议，再加上一些其他控制信息，构建一个 IP 数据包。接下来，需要加入 MAC 头。如果在本地 ARP 映射表中查找出 IP 地址 192.168.1.2 所对应的 MAC 地址，则可以直接使用；如果没有，则需要发送 ARP 协议查询 MAC 地址，获得 MAC 地址后，由数据链路层构建一个数据帧，目的地址是 IP 层传过来的 MAC 地址，源地址则是本机的 MAC 地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。

主机 B 收到这个数据帧后，先检查它的目的 MAC 地址，并和本机的 MAC 地址对比，如符合，则接收，否则就丢弃。接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议。主机 B 会构建一个 ICMP 回送响应消息数据包，回送响应数据包的类型字段为 0，序号为接收到的请求数据包中的序号，然后再发送出去给主机 A。

在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 回送响应消息，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。

#### WebSocket

### Go语言

#### 函数 & 关键字

##### 1. make()、new()、var的区别？

&emsp;&emsp;new的特点

- 分配内存。内存里存的值是对应类型的零值。
- 只有一个参数。参数是分配的内存空间所存储的数据类型，Go语言里的任何类型都可以是new的参数，比如int， 数组，结构体，甚至函数类型都可以。
- 返回的是指针。

注: new在项目中很少见，可以被多种方法替代。

&emsp;&emsp;make的特点

- 分配和初始化内存。
- 只能用于slice, map和chan这3个类型，不能用于其它类型。
- 如果是用于slice类型，make函数的第2个参数表示slice的长度，这个参数必须给值。
- 返回的是原始类型，也就是slice, map和chan，不是返回指向slice, map和chan的指针。

&emsp;&emsp;为什么针对slice, map和chan类型专门定义一个make函数？这是因为slice, map和chan的底层结构上要求在使用slice，map和chan的时候必须初始化，如果不初始化，那slice，map和chan的值就是零值，也就是nil。我们知道：map如果是nil，是不能往map插入元素的，插入元素会引发panic。chan如果是nil，往chan发送数据或者从chan接收数据会引发panic。slice会有点特殊，理论上slice如果是nil，也是没法用的。但是append函数处理了nil slice的情况，可以调用append函数对nil slice做扩容。但是我们使用slice，总是会希望可以自定义长度或者容量，这个时候就需要用到make。

&emsp;&emsp;new来创建slice, map和chan的都是nil，并没有什么用。

&emsp;&emsp;var的特点

- 声明一个type类型的变量，分配内存空间给type类型的零值。
- 声明一个type类型的指针变量，不会分配内存空间，零值为nil。
- 声明一个type类型的变量，并赋值。

#### 切片

##### 1. 切片的扩容策略？

&emsp;&emsp;切片在扩容时会进行内存对齐，这个和内存分配策略相关。进行内存对齐之后，新 slice 的容量是要 大于等于老 slice 容量的 2倍或者1.25倍，当原 slice 容量小于 1024 的时候，新 slice 容量变成原来的 2 倍；原 slice 容量超过 1024，新 slice 容量变成原来的1.25倍。

#### 内存

##### 逃逸分析

##### 内存分配

##### 垃圾回收

### 数据库知识

#### MySQL

#### Redis

#### NoSQL

##### 1. NoSQL的优缺点？

&emsp;优点

- 高可扩展性
- 分布式计算
- 低成本
- 架构的灵活性，半结构化数据
- 没有复杂的关系

&emsp;缺点

- 没有标准化
- 有限的查询功能（到目前为止）
- 最终一致是不直观的程序

### 分布式

##### 1. 分布式计算的优缺点有哪些？

&emsp;优点

- 可靠性（容错）：分布式计算系统中的一个重要的优点是可靠性。一台服务器的系统崩溃并不影响到其余的服务器。
- 可扩展性：在分布式计算系统可以根据需要增加更多的机器。
- 资源共享：共享数据是必不可少的应用，如银行，预订系统。
- 灵活性：由于该系统是非常灵活的，它很容易安装，实施和调试新的服务。
- 更快的速度：分布式计算系统可以有多台计算机的计算能力，使得它比其他系统有更快的处理速度。
- 开放系统：由于它是开放的系统，本地或者远程都可以访问到该服务。
- 更高的性能：相较于集中式计算机网络集群可以提供更高的性能（及更好的性价比）。

&emsp;缺点

- 故障排除：故障排除和诊断问题。
- 软件：更少的软件支持是分布式计算系统的主要缺点。
- 网络：网络基础设施的问题，包括：传输问题，高负载，信息丢失等。
- 安全性：开放系统的特性让分布式计算系统存在着数据的安全性和共享的风险等问题。

### 工具

#### Docker

#### Kubernetes

#### Git
