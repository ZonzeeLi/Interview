## 面试问题分类

### 操作系统

### 计算机网络

#### 基础内容

##### 1. TCP/IP 网络模型有几层？

- 应用层 : 为用户提供功能，比如HTTP、FTP、Telnet、DNS、SMTP等，当两个不同的设备需要通信，应用把数据传递给下一层。
- 传输层 : 为应用层提供网络支持，传输层的两种传输协议TCP和UDP。设备作为发送方，应用把数据给传输层再到下一层，作为接收方则反过来。
- 网络层 : 将数据从一个设备传输到另一个设备。最常使用的是IP协议。
- 网络接口层 : 为网络层提供「链路级别」传输的服务，负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。

##### 2. 键入网址到网页显示，期间发生了什么？

##### 3. Linux系统是如何收发网络包的？

###### 3.1 网络模型

OSI 七层模型 : 应用层、表示层、会话层、传输层、网络层、数据链路层、物理层
TCP/IP 四层模型 : 应用层、传输层、网络层、网络接口层

###### 3.2 Linux接收网络包的流程

当网卡接收到网络包后，通过DMA技术，将网络包放到Ring Buffer，当网络包到达，网卡发起硬件中断，执行中断处理函数，发出一个「软中断」来轮询处理数据，直到没有新数据恢复中断，一次中断处理多个网络包。

软中断处理网络包是从 Ring Buffer 中拷贝数据到内核 struct sk_buff 缓冲区中，从而可以作为一个网络包交给网络协议栈进行逐层处理。

首先，进入网络接口层，检查报文合法性，合法找到上层协议类型，比如是IPv4，或者是IPv6，去掉帧头和帧尾，到网络层。

取出IP包，判断网络包下一步是继续上层处理还是转发出去，如果网络包是发送给本机的，就会从IP头里查看再上一层使用的是TCP还是UDP，去掉IP头，到传输层。

取出TCP或UDP头，根据「源 IP、源端口、目的 IP、目的端口」找出对应的Socket，将数据拷贝到Socket的接收缓冲区。

最后，应用层调用Socket接口，从内核的Socket接收缓冲区读取新数据到应用层。

###### 3.3 Linux发送网络包的流程

和上述接收网络包的过程正好相反。

##### 4. TCP和UDP的区别？

#### HTTP

##### 1. HTTP 基本概念

###### 1.1 HTTP 是什么？

HTTP 是超文本传输协议。是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。

###### 1.2 HTTP 常见的状态码

- 200 OK : 常见的成功状态码，如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。
- 204 No Content : 常见的成功状态码，响应头没有 body 数据。
- 206 Partial Content : 是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。
- 301 Moved Permanently : 永久重定向。
- 302 Found : 临时重定向。说明请求的资源还在，但暂时需要用另一个 URL 来访问。301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。
- 304 Not Modified : 不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。
- 400 Bad Request : 客户端请求的报文有误。
- 403 Forbidden : 服务器禁止访问。
- 404 Not Found : 请求的资源在服务器上不存在或未找到，所以无法提供给客户端。
- 500 Internal Server Error : 服务器发生错误。
- 501 Not Implemented : 客户端请求的功能还不支持。
- 502 Bad Gateway : 通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。
- 503 Service Unavailable : 服务器当前很忙，暂时无法响应客户端。

###### 1.3 HTTP 常见的字段

- Host 字段 : 服务器的域名。
- Content-Length 字段 : 回应的数据长度。
- Connection 字段 : 最常用的是要求服务器使用 TCP 的持久连接，即 Keep-Alive 。HTTP/1.1 版本的默认连接都是持久连接。
- Content-Type 字段 : 用于服务器回应时，告诉客户端，本次数据是什么格式。
- Content-Encoding 字段 : 数据的压缩方法。表示服务器返回的数据使用了什么压缩格式。

##### 2. GET 与 POST

###### 2.1 GET 与 POST 的区别

GET 的语义是从服务器获取指定的资源，一般是写在 URL 中，浏览器会对 URL 的长度有限制。POST 的语义是根据请求负荷(报文 body )对指定的资源作出处理，不会对 body 大小做出限制。

###### 2.2 GET 和 POST 方法都是安全和幂等的吗？

安全 : 不会破坏服务器上的资源。
幂等 : 多次执行相同的操作，结果都是相同的。

- GET : 安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签。
- POST : 是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。所以，浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签。

不过实际过程中可能使用 GET 来实现新增或删除，POST 查询数据，这样的话就不一定了，而且虽然 POST 是用 body 传输数据，但是抓个包就可以看到数据，也不一定就比 GET 方法安全，所以要用 HTTPS 协议加密传输。

##### 3. HTTP 缓存技术

###### 3.1 强制缓存

强制缓存指的是只要浏览器判断缓存没有过期就直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。

强缓存使用的是两个 HTTP 响应头部字段实现，用来表示资源在客户端缓存的有效期。

- Cache-Control : 是一个相对时间。
- Expires : 是一个绝对时间。

如果头部同时存在这两个字段的话，Cache-Control 的优先级高于 Expires 。

Cache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：

- 当浏览器第一次请求访问服务器资源时，服务器返回资源的同时，响应头部会添加上 Cache-Control，这里设置了过期时间的大小。
- 当浏览器再次请求访问服务器资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，计算是否过期，如果没有，则使用缓存，否则重新请求。
- 服务器再次收到请求后，会更新响应头部的 Cache-Control。

###### 3.2 协商缓存

协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。

协商缓存基于两种头部来实现，第一种是请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段。

- 响应头部中的 Last-Modified : 表示这个响应资源的最后修改时间。
- 请求头部中的 If-Modified-Since : 当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上  Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行比较，如果最后修改时间较新，说明资源被修改过，返回最新资源；如果最后修改时间较早，说明无新修改，返回 304 走本地缓存。

第二种是请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段实现。

- 响应头部中的 Etag : 唯一标识响应资源。
- 请求头部中的 If-None-Match : 当本地缓存资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发送请求时，会将请求头中 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304 状态码，如果有变化返回 200。

第一种是基于时间实现的，第二种是基于唯一标识，相对来说后者更加准确的判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。

如果 HTTP 响应头部中同时存在 Etag 和 Last-Modified 字段的时候，Etag 的优先级更高，先判断 Etag 是否发生了变化，如果没有变化，则继续看 Last-Modified。

注意，协商缓存的字段需要配合强制缓存中 Cache-control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求。

使用 Etag 字段实现的协商缓存过程如下 : 

- 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在响应头部添加 Etag 唯一标识，这个唯一标识的值是根据当前请求的资源生成。
- 当浏览器再次请求访问服务器该资源时，首先检查强制缓存是否国企，如果没过期则直接使用本地缓存；如果过期，则在请求头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识。
- 服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较：
  - 如果值相等，则返回 304 Not Modified，不会返回资源；
  - 如果不相等，则返回 200 状态码和资源，并在响应头部加上新的 ETag 唯一标识；
- 如果浏览器收到的是 304 请求响应状态码，则会从本地缓存中加载资源，否则更新。

##### 4. HTTP 特性

###### 4.1 HTTP 1.1 的优点有哪些？

1. 简单

HTTP 的基本报文格式是 header + body ，头部信息也是 key-value 简单文本的形式，易于理解，降低了学习的门槛。

2. 灵活和易于扩展

HTTP 协议里的各类请求方法、URI / URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。

同时 HTTP 由于是工作在应用层( OSI 第七层)，则它下层可以随便变化。

HTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL / TLS 安全传输层，HTTP / 3把 TCP 层换成了基于 UDP 的 QUIC。

3. 应用广泛和跨平台

互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有跨平台的优越性。

###### 4.2 HTTP 1.1 的缺点有哪些？

1. 无状态双刃剑

无状态的好处，因为服务器不会记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。

无状态的坏处，既然服务器没有记忆能力，再完成有关联性的操作会很麻烦，比如一套购物流程，从登陆到下单到付款等，都需要验证用户身份，如果服务器不知道这些请求是关联的，每次都要验证一遍。比较简单的解决这一问题的方法就是 Cookie 技术。

Cookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。相当于，在客户端第一次请求后，服务器下发一个带有客户信息的字段，下次客户端请求服务器，带上该字段，服务器就能认得了。

2. 明文传输双刃剑

明文意味着传输中的信息是可方便阅读的，通过 F12 控制台或者抓包可以直接查看，为调试带来了极大的便利性。

不过这样也相当于信息裸奔，在传输的过程中很容易被窃取。

3. 不安全

HTTP 比较严重的缺点就是不安全 : 

- 通信使用明文，内容会被窃听。
- 不验证通信放的身份，会遭遇伪装。
- 无法验证报文的完整性，有可能被篡改。

###### 4.3 HTTP 1.1 的性能如何？

1. 长连接

早期 HTTP 1.0 性能上的一个很大问题，就是没发一次请求，都要新建一次 TCP 连接，而且是串行请求，做了无所谓的 TCP 连接和断开，增加了通信的开销。

HTTP 1.1 提出了长连接的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。

持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。

如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开。

2. 管道网络传输

在可在同一个 TCP 连接里面，客户端发起多个请求，只要第一个请求发出去了，不必等其回来，可以发送第二个请求出去，减少整体的响应时间。

但是服务器必须按照接收请求的顺序发送对这些管道化请求的响应。如果服务端在处理 A 请求耗时比较长，那么后续的请求处理都会被阻塞住，也就是队头阻塞。HTTP 1.1 解决了请求的队头阻塞，但是并没有解决响应的队头阻塞。

3. 队头阻塞

「请求 - 应答」的模式加剧了 HTTP 的性能问题。当顺序发送的请求序列，某一个请求因为某种原因被阻塞，后面的排队的请求就会一直被阻塞，这就是队头阻塞，类似于上班塞车。

##### 5. HTTP 与 HTTPS 

###### 5.1 HTTP 与 HTTPS 有哪些区别？

1. HTTP 是超文本传输协议，信息是明文传输，存在安全问题。HTTPS 则解决了 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL / TLS 安全协议，让报文加密传输。
2. HTTP 连接建立比较简单，TCP 三次握手之后即可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后要进行 SSL / TLS 握手过程，再进行加密的报文传输。
3. HTTP 的端口号是 80，HTTPS 的端口号是 443。
4. HTTPS 要向 CA（证书权威机构）申请数字证书，来保证服务器的身份可信。

###### 5.2 HTTPS 解决了 HTTP 的哪些问题？

HTTP 由于是明文传输，所以存在以下三个风险：

- 窃听风险，比如通信链路上可以获取通信内容。
- 篡改风险，比如强制植入垃圾广告。
- 冒充风险，比如冒充淘宝网站。

HTTPS 在 HTTP 和 TCP 之间加入了 SSL/TLS 协议，解决了以上问题。

- 信息加密 : 交互信息无法被窃取。
- 校验机制 : 无法篡改通信内容，篡改了就不能正常显示。
- 身份证书 : 证明网站时真正要浏览的网站。

###### 5.3 HTTPS 时如何解决以上的三个风险问题？

- 混合加密的方式实现了信息的机密性，解决了窃听的风险。
- 摘要算法的方式实现了完整性，为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。
- 将服务器公钥放入到数字证书中，解决了冒充的风险。

1. 混合加密

HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：

- 在通信建立之前使用非对称加密的方式交换「会话秘钥」，后续不再使用非对称加密。
- 在通信的过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。

采用「混合加密」的原因：

- 对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。
- 非对称加密使用两个密钥，公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换的问题，但是速度会慢。

2. 摘要算法

摘要算法实现完整性，客户端发送明文前通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。

3. 数字证书

客户端向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用私钥解密。借助第三方权威机构 CA（数字证书认证机构），将服务器的公钥放在数字证书中，只要证书是可信的，公钥就是可信的。

###### 5.4 HTTPS 是如何建立连接的？其间交互了什么？

SSL / TLS 协议基本流程 :

- 客户端向服务器索要并验证服务器的公钥。
- 双方协商生产「会话秘钥」。
- 双方采用「会话秘钥」进行加密通信。

SSL / TLS 协议详细流程 :

1. ClientHello 

首先，客户端向服务器发起加密通信请求，即 ClientHello 请求。客户端主要发送以下信息给服务器：
（1）客户端支持的 SSL / TLS 协议版本。
（2）客户端生成的随机数 ( Client Random )，后面用户生成会话密钥条件之一。
（3）客户端支持的密码套件列表。

2. SeverHello

服务器收到客户端请求后，向客户端发出响应，即 SeverHello 。服务器回应的内容如下：
（1）确认 SSL / TLS 协议版本，如果浏览器不支持，则关闭加密通信。
（2）服务器生成的随机数 ( Server Random )，也是后面用户生产「会话秘钥」条件之一。
（3）确认的密码套件列表。
（4）服务器的数字证书。

3. 客户端回应

客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。

如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：
（1）一个随机数 ( pre-master key )。该随机数会被服务器公钥加密。
（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（3）客户端握手结束同值，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。

上面第一项的随机数是整个握手阶段的第三个随机数，会发送给服务端，所以这个随机数客户端和服务端都是一样的。

服务端和客户端有了这三个随机数 ( Client Random、Server Random、pre-master key )，接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。

4. 服务器的最后回应

服务器收到客户端的第三个随机数 ( pre-master key ) 之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后向客户端发送最后的信息如下：
（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内瓤的发生的数据做个摘要，用来供客户端校验。

至此，整个 SSL / TLS 的握手阶段全部结束。接下来客户端与服务器进入加密通信，使用的是普通的 HTTP 协议，只不过用「会话秘钥」加密内容。

![HTTPS流程图（图源小林coding）](https://github.com/ZonzeeLi/Interview/blob/master/picture/HTTPS%E6%B5%81%E7%A8%8B.png)

##### 6. HTTP /1.1、HTTP /2、HTTP /3 演变

###### 6.1 HTTP /1.1 相比 HTTP /1.0 提高了什么性能？

HTTP /1.1 相比 HTTP /1.0 性能上的改进：

- 使用 TCP 长连接的方式改善了 HTTP /1.0 短连接造成的性能开销。
- 支持管道( pipeline )网络传输，只要第一个请求发出去了，不必等其回来，就可以发送第二个请求，减少整体的响应时间。

HTTP /1.1 的性能瓶颈：

- 请求 / 响应头部 ( Header ) 未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 部分
- 发送冗长的首部。每次互相发送相同的首部造成过多浪费。
- 服务器是按请求的顺序响应的，如果服务器响应慢，则会招致客户端一直请求不到数据，也就是队头阻塞。
- 没有请求优先级控制。
- 请求只能从客户端开始，服务器只能被动响应。

###### 6.2 HTTP /2 做了什么优化？

HTTP /2 协议是基于 HTTPS 的，所以 HTTP /2 的安全性也是有保障的。

HTTP /2 相比 HTTP /1.1 性能上的改进：

1. 头部压缩

HTTP /2 会压缩头 ( Header )，如果同时发出多个请求，他们的头是一样的或者详细的，那么协议会消除重复的部分。

这就是 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高了速度。

2. 二进制格式

HTTP /2 不再像 HTTP /1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧 ( frame )：头信息帧 ( Headers Frame ) 和数据帧 ( Data Frame )。

这样虽然对人不友好，但是计算机只懂二进制，无需将明文的报文再转成二进制，而是直接解析，增加了数据传输的效率。

3. 数据流

HTTP /2 的数据包不是按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。

在 HTTP /2 中每个请求或响应的所有数据包，称为一个数据流 ( Stream )。每个数据流都标记着一个独一无二的编号 ( Stream ID )，不同的 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息。

客户端和服务器双方都可以建立 Stream，Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。

客户端还可以指定数据流的优先级，优先级高的请求，服务器就先响应该请求。

4. 多路复用

HTTP /2 是可以在一个连接中并发多个请求或响应，而不用按照顺序一一对应。

移除了 HTTP /1.1 中的串行请求，不需要排队等待，也就不会再出现「队头阻塞」问题，降低了延迟，大幅度提高了连接的利用率。

5. 服务器推送

HTTP /2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以主动向客户端发送消息。

HTTP /2 有什么缺陷：

HTTP /2 通过 Stream 的并发能力，解决了 HTTP /1 的队头阻塞的问题，看似很完美了，但是 HTTP /2 还是存在该问题，不过不是发生在 HTTP 这一层面，而是 TCP 这一层。

HTTP /2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP /2 队头阻塞问题。

所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。

###### 6.3 HTTP /3 做了哪些优化？

上述说明了 HTTP /1.1 和 HTTP /2 都有队头阻塞的问题：

- HTTP /1.1 中的管道 ( pipeline ) 虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按照顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应处理完这个请求后，才能处理下一个请求，属于 HTTP 层的队头阻塞。
- HTTP /2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。

HTTP /3 把 HTTP 下层的 TCP 协议改成了 UDP。

UDP 不管顺序，也不管丢包，不会出现 HTTP /2 队头阻塞的问题。基于 UDP 的 QUIC 协议可以实现类似 TCP 的可靠性传输。

QUIC 有以下3个特点：

1. 无队头阻塞

QUIC 协议也有类似 HTTP /2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为是一条 HTTP 请求。

QUIC 有自己的一套机制可以保证传输的可靠性。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。这与 HTTP /2 不同，HTTP /2 只要某个流中的数据包丢失了，其他流也会受到影响。

所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受到影响。

2. 更快的连接建立

对于 HTTP /1 和 HTTP /2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此他们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。

HTTP /3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。

但是 HTTP /3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS /1.3，因此仅需要 1 个 RTT 就可以「同时」完成建立连接与密钥协商。甚至在第二次连接的时候，数据包和 QUIC 握手信息 ( 连接信息 + TLS 信息 ) 一起发送，达到 0-RTT 的效果。

3. 连接迁移

基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接，那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。而建立连接的过程中包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络卡顿了一下，因此连接的迁移成本很高。

而 QUIC 协议没有四元组的方式来“绑定”连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此计是移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID，TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿，达到了连接迁移的效果。

所以 QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP /2 的多路复用协议。

##### 7. HTTP /1.1 如何优化？

可以从以下三种优化思路来优化 HTTP /1.1 协议：

- 尽量避免发送 HTTP 请求
- 在需要发送 HTTP 请求时，考虑如何减少请求次数
- 减少服务器的 HTTP 响应的数据大小

###### 7.1 如何避免发送 HTTP 请求？

对于一些重复性的 HTTP 请求，比如每次请求得到的数据都一样的，可以把这些「请求-响应」的数据都缓存在本地，下次直接读取本地的数据，不必通过网络获取服务器的响应。所以避免 HTTP 请求的方法就是通过缓存技术。

客户端会把第一次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，而响应作为 value，两者形成映射关系。

服务器在发送 HTTP 响应时，会估算一个过期的时间，并且把这个信息放到响应头部，客户端在查看响应头部的信息时，一旦发送缓存的响应是过期的，则就会重新发送网络请求。而如果客户端第一次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，服务器上的资源没有变更，则服务器没有必要在响应中再带上这个资源。只需要客户端在重新发送请求时，在请求的 Etag 头部带上第一次请求响应的头部中的摘要，这个摘要时唯一标识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做比较。如果不同，说明客户端的缓存没有价值，重新带上资源，而如果相同，服务器返回不带有 body 的 304 Not Modified 响应，告诉客户端仍然有效，减少响应资源在网络中传输的延时。缓存真的是性能优化的一把万能钥匙，小到 CPU Cache、Page Cache、Redis Cache，大到 HTTP 协议的缓存。

###### 7.2 如何减少 HTTP 请求次数？

#### WebSocket

### Go语言

#### 函数 & 关键字

##### 1. make()、new()、var的区别？

&emsp;&emsp;new的特点

- 分配内存。内存里存的值是对应类型的零值。
- 只有一个参数。参数是分配的内存空间所存储的数据类型，Go语言里的任何类型都可以是new的参数，比如int， 数组，结构体，甚至函数类型都可以。
- 返回的是指针。

注: new在项目中很少见，可以被多种方法替代。

&emsp;&emsp;make的特点

- 分配和初始化内存。
- 只能用于slice, map和chan这3个类型，不能用于其它类型。
- 如果是用于slice类型，make函数的第2个参数表示slice的长度，这个参数必须给值。
- 返回的是原始类型，也就是slice, map和chan，不是返回指向slice, map和chan的指针。

&emsp;&emsp;为什么针对slice, map和chan类型专门定义一个make函数？这是因为slice, map和chan的底层结构上要求在使用slice，map和chan的时候必须初始化，如果不初始化，那slice，map和chan的值就是零值，也就是nil。我们知道：map如果是nil，是不能往map插入元素的，插入元素会引发panic。chan如果是nil，往chan发送数据或者从chan接收数据会引发panic。slice会有点特殊，理论上slice如果是nil，也是没法用的。但是append函数处理了nil slice的情况，可以调用append函数对nil slice做扩容。但是我们使用slice，总是会希望可以自定义长度或者容量，这个时候就需要用到make。

&emsp;&emsp;new来创建slice, map和chan的都是nil，并没有什么用。

&emsp;&emsp;var的特点

- 声明一个type类型的变量，分配内存空间给type类型的零值。
- 声明一个type类型的指针变量，不会分配内存空间，零值为nil。
- 声明一个type类型的变量，并赋值。

#### 切片

##### 1. 切片的扩容策略？

&emsp;&emsp;切片在扩容时会进行内存对齐，这个和内存分配策略相关。进行内存对齐之后，新 slice 的容量是要 大于等于老 slice 容量的 2倍或者1.25倍，当原 slice 容量小于 1024 的时候，新 slice 容量变成原来的 2 倍；原 slice 容量超过 1024，新 slice 容量变成原来的1.25倍。

### 数据库知识

#### MySQL

#### Redis

#### NoSQL

##### 1. NoSQL的优缺点？

&emsp;优点

- 高可扩展性
- 分布式计算
- 低成本
- 架构的灵活性，半结构化数据
- 没有复杂的关系

&emsp;缺点

- 没有标准化
- 有限的查询功能（到目前为止）
- 最终一致是不直观的程序

### 分布式

##### 1. 分布式计算的优缺点有哪些？

&emsp;优点

- 可靠性（容错）：分布式计算系统中的一个重要的优点是可靠性。一台服务器的系统崩溃并不影响到其余的服务器。
- 可扩展性：在分布式计算系统可以根据需要增加更多的机器。
- 资源共享：共享数据是必不可少的应用，如银行，预订系统。
- 灵活性：由于该系统是非常灵活的，它很容易安装，实施和调试新的服务。
- 更快的速度：分布式计算系统可以有多台计算机的计算能力，使得它比其他系统有更快的处理速度。
- 开放系统：由于它是开放的系统，本地或者远程都可以访问到该服务。
- 更高的性能：相较于集中式计算机网络集群可以提供更高的性能（及更好的性价比）。

&emsp;缺点

- 故障排除：故障排除和诊断问题。
- 软件：更少的软件支持是分布式计算系统的主要缺点。
- 网络：网络基础设施的问题，包括：传输问题，高负载，信息丢失等。
- 安全性：开放系统的特性让分布式计算系统存在着数据的安全性和共享的风险等问题。

### 工具

#### Docker

#### Kubernetes

#### Git
