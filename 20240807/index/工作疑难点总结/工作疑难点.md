# 工作疑难点

## 直播 C 端

### 直播间库存刷新

线上会出现问题，商品库存不为空，到提单流程会提示已抢光，最终问题就是库存刷新不及时。

第一版：监听商品信息表的库存值变更，如果发现库存为 0，则发送 IM 通知客户端刷新。

第二版：监听数据上游（小店 B）的 bp 消息队列，从挂接表获取最近 1 天的直播间，过滤获取正在直播的直播间，保留这部分直播间，写入一个 redis zset keyA。——> 离线脚本，从 keyA 中获取 5 条最旧的数据，获取商品类型去调用专门的库存接口。

![直播C直播间库存流程](../image/直播C直播间库存流程.png)

- 如果库存为 0，设置库存曾经为 0 的缓存，发送 IM。
- 如果库存不为 0，查看缓存库存是否曾经为 0 过，是的话删除曾经为 0 的缓存，并且发送 IM，不是的话直接结束。

上述描述完刷新商品列表的缓存（C 端获取商品信息从这里获取），之前的方案是发送 IM 让客户端重新请求一次刷新，现在更快的方案是发送 IM，客户端直接修改库存值。

其实最主要的情况有如下几种：

- 库存 99 ——> 库存 98：这种情况其实没有什么影响，因为从 C 端来看只关心是否有库存。
- 库存 1 ——> 库存 0：这种情况的状态从用户感知应该是有库存变为 0 已抢光，所以如果当前库存变为 0，设置曾经为 0 过，发送 IM 通知变更为已抢光。
- 库存 0 ——> 库存 1：这种情况是相当于上架，如果查看到之前为 0 过说明，之前的状态为已抢光，这时候要删除这个缓存，并且通知 IM 变更已经上架。如果之前没有过，则说明并不是从 0 变为 1 的，类似于上面的 99 变为 98，这种不需要通知，定期刷新即可。

### 明星直播间（高流量直播间）

> 以下内容为理论性概述，实际业务要考虑实际场景为准，部分 demo 代码和讲解在 C 端保障预案分节里

1. 针对直播电商部分入场、优惠券、讲解卡挂件等不重要且下游链路复杂、qps限制、平响高等功能进行下线。
2. 针对部分重要数据但无法完全降级的功能，脚本定时刷新从缓存中获取数据直接写入到本地缓存中，接口再从本地缓存获取数据。
3. 缓存异步更新（调用下游），减少异步请求量级，加写锁控制。
4. 针对部分重要数据但需要从下游获取数据的，用 singleflight 包拦截高并发请求。——> 因为（某种原因，好像是读 redis 会被拒绝，因为没有 forget key，所以会导致 goroutine 激增），改成了读本地缓存，即步骤2。
5. 直播间库存刷新改造方案 ——> 最终通知 IM，端上直接改库存。

### C 端保障预案

2. 扩缩
3. 熔断、降级
4. 流量打散
5. 
#### 压测（待和 op 补充）


#### 流量打散

针对部分数据下游，比如 Redis、DB 这种需要频繁打倒机器上 get 数据的情况，在预案演练及不考虑扩容下游的情况，需要对一些 hot key、get 请求等进行分散处理，这里主要以 Redis 为例。

如果考虑的 Redis 本身的机器实例，可以使用一致性 Hash 等方法（在不考虑 Redis 本身集群的负载均衡策略情况下，此前置兜底由 C 端代码自己处理），请求到不同的 Redis 实例上。

> 可以了解下一致性 Hash 的原理，记得是个环形数组

```go
// demo

type HashRing struct {
	nodes    []int
	nodeMap  map[int]string
	replicas int
	hashFunc func(data []byte) uint32
	sync.RWMutex
}

func NewHashRing(replicas int, fn func(data []byte) uint32) *HashRing {
	if fn == nil {
		fn = crc32.ChecksumIEEE
	}
	return &HashRing{
		replicas: replicas,
		hashFunc: fn,
		nodeMap:  make(map[int]string),
	}
}

func (h *HashRing) AddNode(node string) {
	h.Lock()
	defer h.Unlock()
	for i := 0; i < h.replicas; i++ {
		hash := int(h.hashFunc([]byte(strconv.Itoa(i) + node)))
		h.nodes = append(h.nodes, hash)
		h.nodeMap[hash] = node
	}
	sort.Ints(h.nodes)
}

func (h *HashRing) GetNode(key string) string {
	h.RLock()
	defer h.RUnlock()
	if len(h.nodes) == 0 {
		return ""
	}
	hash := int(h.hashFunc([]byte(key)))
	idx := sort.Search(len(h.nodes), func(i int) bool {
		return h.nodes[i] >= hash
	})
	if idx == len(h.nodes) {
		idx = 0
	}
	return h.nodeMap[h.nodes[idx]]
}

var ctx = context.Background()

func main() {
	// 初始化一致性哈希环
	hashRing := NewHashRing(100, nil)
	redisNodes := []string{"redis1:6379", "redis2:6379", "redis3:6379"}
	for _, node := range redisNodes {
		hashRing.AddNode(node)
	}

	// 模拟高并发请求
	keys := []string{"key1", "key2", "key3", "key4", "key5"}
	for _, key := range keys {
		// 获取 key 对应的节点
		node := hashRing.GetNode(key)
		client := redis.NewClient(&redis.Options{
			Addr: node,
		})
		// 这里用 Get 来举例，如果 Set 同样和 Get 是一个 Node
		val, err := client.Get(ctx, key).Result()
		if err != nil {
			fmt.Printf("Error getting key %s from node %s: %v\n", key, node, err)
		} else {
			fmt.Printf("Got value %s for key %s from node %s\n", val, key, node)
		}
	}
}
```

hot key 的处理方法也一样，可以将 hot key 分成 hot key1、hot key2... 这种，分散到多个 redis 分片当中，这样随机的取一个实例，可以减少数据倾斜及 hot key 对 redis 分片的压力，
大型活动保障的演练前，和 OP 做提前预案可以做到快速的扩容减少 hot key 的影响。

而高并发 + hot key 的场景，要考虑多副本（多分片） + 多级缓存（本地缓存）的设计，以 C 端场景为例，比如获取 C 端商品展示的商品信息，从 DB 中写到 Redis，C 端逻辑从 Redis 再写到本地，本地设置过期时间，远程 Redis 也有过期时间，这样防止缓存击穿及 Redis 节点的压力。C 端代码使用的是互斥锁和双重 check 的机制，如果使用 singleflight 包可能会有泄漏问题，比如 10000 个请求打过来放开的那个请求失败了，会重新建立 1 个连接，这样端起的失败会导致内存激增，goroutine 泄漏，所以目前 C 端使用的方法如下 demo：

```go
// 互斥锁+双重check机制 demo

var (
	cache      = make(map[string]string)
	cacheMutex sync.Mutex
)

func getFromCache(key string) (string, bool) {
	cacheMutex.Lock()
	defer cacheMutex.Unlock()
	value, exists := cache[key]
	return value, exists
}

func setToCache(key, value string) {
	cacheMutex.Lock()
	defer cacheMutex.Unlock()
	cache[key] = value
}

func getFromDB(key string) string {
	// 模拟数据库访问
	time.Sleep(100 * time.Millisecond)
	return "value_from_db"
}

func getValue(key string) string {
	// 先从缓存中获取
	if value, exists := getFromCache(key); exists {
		return value
	}

	// 缓存中没有，获取锁
	cacheMutex.Lock()
	defer cacheMutex.Unlock()

	// 双重检查，防止重复查询数据库
	if value, exists := cache[key]; exists {
		return value
	}

	// 从数据库中获取
	value := getFromDB(key)
	// 设置到缓存中
	setToCache(key, value)
	return value
}

func main() {
	key := "key1"
	// 模拟高并发访问
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			fmt.Println(getValue(key))
		}()
	}
	wg.Wait()
}
```

同时加上预热缓存，即定时从远端缓存刷到本地缓存上，尽量减少本地和远端缓存的一致性问题。C 端代码使用 crontab 定时控制从远端缓存获取数据，并校验比对过期时间及其他处理逻辑，写入到本地缓存中。

```go
// demo

func preheatCache(key string) {
	for {
		time.Sleep(5 * time.Second) // 假设缓存有效期为10秒，每5秒预热一次
		value := getFromDB(key)
		setToCache(key, value)
	}
}
```

## 直播 B 端

### 数字大屏接口可用性问题

## 数字人 B 端